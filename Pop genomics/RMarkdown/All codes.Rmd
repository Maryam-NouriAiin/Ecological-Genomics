---
title: "All codes"
output: 
  html_document:
    code_folding: show
    theme:
      bg: "black"
      fg: "white"
      primary: "rosybrown"
      secondary: "thistle"
      base_font:
        google: Prompt
      heading_font:
        google: Proza Libre
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
if (requireNamespace("thematic")) 
  thematic::thematic_rmd(font = "auto")
```

## Bash & R

## Codes {.tabset .tabset-pills}

### Visualize

```         
path: /netfiles/ecogen/PopulationGenomics/fastq/red_spruce
cd
ll
```

Naming convention for spruce data set:
<PopCode><RowID><ColumnID>\_<ReadDirection>.fast.gz

***fastq file***: the standard sequence data format for NGS contains: -
the sequence of the read - corresponding quality scores for each base -
meta-data about the read

*.gz*: compressed files (gzipped)

*zact*: peek inside a file

```         
zcat 2505_9_C_R2.fastq.gz | head -n 4

@A00354:455:HYG3FDSXY:1:1101:3893:1031 2:N:0:CATCAAGT+TACTCCTT
GTGGAAAATCAAAACCCTAATGCTGAAAGGAATCCAAATCAAATAAATATTTTCACCGACCTGTTTCGATGCCAGAATTGTCTGCGCAGAAGACTCGTGAAATTTCGCCAGCAGGTAAAATTAAAAGGCTAGAATTAACCGCTGAAATGGA
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F:F
```

Phred quality score: 10 1 in 10 90% 20 1 in 100 99% 30 1 in 1000 99.9%
40 1 in 10,000 99.99%

```         
Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI
                   |         |         |         |         |
    Quality score: 0........10........20........30........40  
```

***FastQC program***: looks at the quality collectively across all reads
in a sample

cd back to our home directories: *\~/*

### Making dictionaries

Make 3 directories to store: data, scripts, and results

```         
mkdir mydata/
mkdir myscripts/
mkdir myresults/
```

To look which dictionary you are in: e.g.

```         
cd /myresults/
pw
#how it should look like 👇
[kellrlab@ecogen myresults]$ pwd
/users/k/e/kellrlab/myresults
```

Make a folder for outputs from our QC analysis

```         
mkdir fastqc/
```

### Visualize the quality of raw data (Program: FastQC)

Run *FastQC* to look at the quality of our sequencing

```         
fastqc filename.fastq.gz -o outputdirectory/
# generate an .html output file for each input file
```

### Filezilla

Transfer the folder \~/myresults/fastqc/ over to your laptop
Double-clicking the html outputs saved in your laptop to open a web
browser

### Clean & trim reads using **fastp**

fastp program: clean the reads for each file, removing low quality bases
and leftover Illumina sequence adapters from the sequences

*Trimmomatic*: anothoer program for trimming, slower that *fastp*

```         
#fastp is installed on the server
/data/popgen/fastp
```

### bash script containg aloop

Writing a bash script that contains a **loop**

Basic syntax of a bash loop: Use of variable assignment using *\${}*

```         
cd <path to the input data>

for FILE in somelist
do

  command1 -options ${FILE} -moreOptions
  command2 -options ${FILE} -moreOptions

done
```

In the for loop: Variable of interest (FILE): \${FILE} Use the wildcard
character (\*): call all files that include the ID code for your
population and then pass those filenames in a loop to the commands

write a loop that process all the R1 fastq files in the population of
interest:

```         
MYPOP="2XXX"
cd <path to the input data>
for FILE in ${MYPOP}*R1.fastq.gz

do

  command1 -options ${FILE} -moreOptions
  command2 -options ${FILE} -moreOptions

done
```

Partially completed example script:

1.  Make a copy of the script cp and put it into your *\~/myscripts*
    directory
2.  Open and edit your copied script using the program *vim vim
    \~/myscripts/fastp.sh.*
3.  Edit the file so that you're trimming the fastq files for the
    population code assigned to you; you can add annotations as notes to
    yourself using the hashtag (\#)
4.  Save the file after you're done making changes. To do this, hit to
    get out of *"Insert"* mode, then *:wq* This will write (w) the
    changes to your file and then quit (q) vim.

```         
cp ~/myscripts
vim ~/myscripts/fastp.sh
:wq
```

fastp needs both read pairs (i.e., the R1 and R2 files). Create a
mathcing R2 file name within the loop: name the output files adding
"\_clean" to the end

### Verify a file contents

```         
head
cat
vim
```

### Execute bash script for fastp.sh

```         
cd ~/myscripts/
bash fastp.sh
# output (the trimmed and cleaned reads) are saved in /netfiles/ecogen/PopulationGenomics/fastq_red_spruce/cleanreads
```

### Assess pre- and post trimming

fastp: fast, produces a summary of the change in quality pre- and
post-trimming. Like FastQC, the output is in an html file, use FileZilla
to transfer back to our laptops and look at in a browser.

### Mapping cleaned and trimmed reads against the reference genome

Step 1: Downloading the reference genome using *wget* to ownload files
from the web

Step 2: subsetted the reference to include just the contigs that contain
one or more probes from our exome capture experiment. For this, we did a
BLAST search of each probe against the P. abies reference genome, and
then retained all scaffolds that had a best hit. This reduced reference
contains: 1,376,182,454 bp (\~1.38 Gbp) in 33,679 contigs The mean
(median) contig size is 10.5 (12.9) kbp The N50 of the reduced reference
is 101,375 bp

Step 3:Writing several short scripts to optimize later Step 4. Using
*bwa* as mapping program Step 5. Write a \*\*\*bash script called
*mapping.sh* to call the R1 and R2 reads for each individual in the
population, and uses the *bwa-mem2* algorithm to map reads to the
reference genome. Step 6. Run *mapping.sh*

bash mapping.sh Don't forget to detach from your screen!

```         
1.
cd /netfiles/ecogen/PopulationGenomics/ref_genome
wget "ftp://plantgenie.org:980/Data/PlantGenIE/Picea_abies/v1.0/fasta/GenomeAssemblies/Pabies01-genome.fa.gz"

2.
#The indexed reduced reference genome to use for your mapping is on our server here:
/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa

3.
MYPOP="XXXX"

#Directory with the cleaned fastq files
INPUT="/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads"

#Output dir to store mapping files (bam)
OUT="/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam"

4 & 5.
#bwa-mem2 command  into a loop to call all the fastq files for our population of interest
/data/popgen/bwa-mem2/bwa-mem2 mem -t 1 ${REF} ${READ1} ${READ2} > ${OUT}${NAME}.sam
where

> t 1 is the number of threads, or computer cpus to use (in this case, just 1)
> ${REF} specifies the path and filename for the reference genome
> ${READ1} specifies the path and filename for the cleaned and trimmed R1 reads 
> ${READ2} specifies the path and filename for the cleaned and trimmed R2 reads 
> ${OUT}/${NAME}.sam  specifies the path and filenam for the .sam file to be saved into a new directory
> Other bwa options detailed here: bwa manual page
> Because you’re each mapping sequences from multiple samples (N=8/pop), it’s going to take a little while.

6.

tmux
bash mapping.sh
> detach from your screen!
```

### *tmux* , detach, & recover screen

*tmux*:initiates a new shell window that won't interrupt or stop your
work if you close your computer

```         
tmux
```

Detach from the screen hold the <control> key down while you hit the b
key. Then release the <control> key and hit just the d key

Recover your screen:

```         
tmux attach
```

### Visualize Sequence AlignMent Files (\*.sam)

SAM file: a tab delimited text file, stores information about the
alignment of reads in a *FASTQ* file to a reference genome or
transcriptome. For each read in a FASTQ file, there's a line in the SAM
file that includes:

-   the read, aka. query, name,
-   a FLAG (number with information about mapping success and
    orientation and whether the read is the left or right read),
-   the reference sequence name to which the read mapped
-   the leftmost position in the reference where the read mapped
-   the mapping quality (Phred-scaled)
-   a CIGAR string that gives alignment information (how many bases
    Match (M), where there's an Insertion (I) or Deletion (D))
-   an '=', mate position, inferred insert size (columns 7,8,9),
-   the query sequence and Phred-scaled quality from the FASTQ file
    (columns 10 and 11),
-   then Lots of good information in TAGS at the end, if the read
    mapped, including whether it is a unique read (XT:A:U), the number
    of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).

```         
/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/

#head and tail.
tail -n 100 FILENAME.sam
```

The left (R1) and right (R2) reads alternate through the file. SAM files
usually have a header section with general information where each line
starts with the '\@' symbol. SAM and BAM files contain the same
information; SAM is human readable\
BAM is in binary code and therefore has a smaller file size.

Find the official Sequence AlignMent file documentation can be found
{here}([https://en.wikipedia.org/wiki/SAM\_(file_format)](https://en.wikipedia.org/wiki/SAM_(file_format)){.uri})
or more
{officially}(<chrome-extension://efaidnbmnnnibpcajpcglclefindmkaj/https://samtools.github.io/hts-specs/SAMtags.pdf>).

Some useful
{FLAGs}(<https://www.seqanswers.com/forum/bioinformatics/bioinformatics-aa/14503-sam-flag-idioms?t=17314>)
to know - for example what do the numbers in the second column of data
mean?

Here's a {SAM FLAG
decoder}(<https://broadinstitute.github.io/picard/explain-flags.html>)
by the Broad Institute.

### Process mapping files using *samtools* and *sambamba*

Process the mapping file \_sam to binary (\*.bam)\_, sort, and remove
duplicate reads *sambamba*: manipulating sam/bam files related to
*samtool*

Calculate mapping statistics to assess quality of the result:

1.  convert sam alignment file to (binary) bam format
2.  sort the bam file by its read coordinates
3.  mark and remove PCR duplicate reads
4.  index the sorted, duplicate removed alignment for quick lookup

```         
sambamba view -S -f bam IN.sam -o OUT.bam
sambamba flagstat FILENAME.bam
```

```         
#customize this script:
/netfiles/ecogen/PopulationGenomics/scripts/process_bams.sh

#copy the script into _myscripts_ folder and _vim_:
cp ~/myscripts
vim

#screen session:
tmux

#execute script:
bash process_bams.sh
@If you get an error that the script doesn’t exist, then either cd into the ~/myscripts directory before running your script, or incorporate the path into the file name when you give your bash command.

#detach:
<CTRL>+b then d

#reattach:
tmux attach-session
```

### Calculate mapping statistics to assess quality of the result

-   Use *samtools* for manipulating sam/bam files
-   *samtools* command *flagstat*: basic info on how well the mapping
    worked
-   Estimate *depth* of coverage (avg. number of reads/site): using the
    samtools command depth
-   Use both of these commands in *loops*: to assess the mapping stats
    on each sample in our population.
-   Use the *awk* tool: to help format the output.

```         
#script to get us started: 
/netfiles/ecogen/PopulationGenomics/scripts/bam_stats.sh

#take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called _tview_:
samtools tview /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/XXXX_XX_X.sorted.rmdup.bam /netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa
```

### Genotype-free population genetics using genotype likelihoods

-   A genotype likelihood (GL): the probability of observing the
    sequencing data (reads containing a particular base), given the
    genotype of the individual at that site.

-   These probabilities are modeled explicitly in the calculation of
    population diversty stats like Theta-pi, Tajima's D, Fst, PCA,
    etc...; thus not throwing out any precious data, but also making
    fewer assumptions about the true (unknown) genotype at each locus

-   Use these approach with program 'ANGSD':'Analysis of Next Generation
    Sequence Data'

### ANGSD work flow

-   Create a list of bam files for the samples you want to analyze
-   Estimate genotype likelihoods (GL's) and allele frequencies after
    filtering to minimize noise Use GL's to: -- estimate the site
    frequency spectrum (SFS) -- estimate nucleotide diversities and
    neutrality stats (Thetas, Tajima's D, ...)

1.  Creat ANGSD.sh file

```         
~/myscripts 
vim ANGSD.sh

mkdir ~/myresults/ANGSD
INPUT=""
OUTPUT=~/myresults/ANGSD
REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"
MYPOP=""
ls ${INPUT}/${MYPOP}*sorted.rmdup.bam >${OUTPUT}/${MYPOP}_bam.list

:wq
ls 
head
cat
vim
```

2.  Open ANGSD in vim

-   Estimate your GL's and allele freqs, optionally filtering for base
    and mapping quality, sequencing depth, SNP probability, minor allele
    frequency, etc.

```         
# File suffix to distinguish analysis choices
SUFFIX=""

# Estimating GL's and allele frequencies for all sites with ANGSD

######################

ANGSD -b ${OUTPUT}/${MYPOP}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${OUTPUT}/${MYPOP}_${SUFFIX} \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-GL 1 \
-doSaf 1 \
##### below filters require `do-Counts`
#-doCounts 1 \
#-minInd 4 \
#-setMinDepthInd 1 \
#-setMaxDepthInd 40 \
#-setMinDepth 10 \
#-skipTriallelic 1 \
#-doMajorMinor 1 \
##### below filters require `doMaf`
#-doMaf 1 \
#-SNP_pval 1e-6 \
#-minMaf 0.01

###to restrict the estimation of the genotype likelihoods to a particular set of sites you’re interested in: add the option _-sites selected_sites.txt_ (tab delimited file with the position of the site in column 1 and the chromosome in column 2) or use _-rf selected_chromosome.chrs_ (if listing just the unique “chromosomes” or contigs you want to anlayze)
###Some popgen stats you want to estimate only the polymorphic sites; for this you should include the -SNP_pval 1e-6 option to eliminate monomorphic sites when calculating your GL’s
###There are good reasons to do it BOTH ways, with and without the __-SNP_pval 1e-6__ option. Keeping the monomorphic sites is essential for getting proper estimates of nucleotide diversity and Tajima’s D. But other analyses such as PCA or GWAS want only the SNPs.

:wq
```

Option Description -nThreads: 1 how many cpus to use -- be conservative
-remove_bads 1: remove reads flagged as 'bad' by samtools -C 50: enforce
downgrading of map quality if contains excessive mismatches -baq 1:
estimates base alignment qualities for bases around indels -minMapQ 20:
threshold for minimum read mapping quality (Phred) -minQ 20: threshold
for minimum base quality (Phred) -GL 1: calculate genotype likelihoods
(GL) using the Samtools formula -doSaf 1: output allele frequency
likelihoods for each site -doCounts 1: output allele counts for each
site -minInd 4: min number of individuals to keep a site (see also ext 2
filters) -setMinDepthInd 1: min read depth for an individual to count
towards a site -setMaxDepthInd 40: max read depth for an individual to
count towards a site -setMinDepth 10: min read depth across ALL
individual to keep a site -skipTriallelic 1: don't use sites with \>2
alleles -doMajorMinor 1: fix major and minor alleles the same across all
samples -doMaf 1: calculate minor allele frequencies -SNP_pval 1e-6:
Keep only site highly likely to be polymorphic (SNPs) -minMaf 0.01: Keep
only sites with minor allele freq \> some proportion.

3.  Create ANGSD_doTheta.sh to estimate the SFS and nucleotide diversity
    stats Based on the saf.idx files from ANGSD GL calls: estimate the
    Site Frequency Spectrum (SFS), which is the precursor to many other
    analyses such as nucleotide diversities (as well as Fst, demographic
    history analysis, etc.)

```         
~/myscripts
vim ANGSD_doTheta.sh
REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

OUT=~/myresults/ANGSD

MYPOP=""

SUFFIX=""

#Estimation of the SFS for all sites using the FOLDED SFS
realSFS ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -maxIter 1000 \
        -tole 1e-6 \
        -P 1 \
        > ${OUT}/${MYPOP}_${SUFFIX}.sfs
```

4.  Estimate the theta diversity stats Once you have the SFS, you can
    estimate the thetas and neutrality stats

```         
# Estimate thetas and stats using the SFS

realSFS saf2theta ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -sfs ${OUT}/${MYPOP}_${SUFFIX}.sfs \
        -outname ${OUT}/${MYPOP}_${SUFFIX}

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx
```

If we wanted to analyze this on sliding windows: instead replace the
above code chunk with the following

```         
# For sliding window analysis:

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx \
-win 50000 \
-step 10000 \
-outnames ${OUT}/${MYPOP}_${SUFFIX}.thetasWindow.gz
```

```         
###For either of the results files above, the first column of the results file (*.thetas.idx.pestPG) is formatted a bit funny and we don’t really need it. We can use the cut command to get rid of it:

cut -f2- ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx.pestPG > ${OUT}/${MYPOP}_${SUFFIX}.thetas
```

Following files should be in ANGSD.sh

```         
[mnouriai@ecogen ~]$ cd ~/myresults/ANGSD
[mnouriai@ecogen ANGSD]$ ll
-rw-r--r--. 1 kellrlab users 1417463962 Sep 20 11:44 9999_.saf.gz
-rw-r--r--. 1 kellrlab users    1086161 Sep 20 11:44 9999_.saf.idx
-rw-r--r--. 1 kellrlab users   79501025 Sep 20 11:44 9999_.saf.pos.gz
```

### Calculate SFS ("site allele frequency") and diversity stats

"saf" files contain "site allele frequency" likelihoods, and are the
info needed to estimate stats that depend on population allele
frequencies, like: - nucleotide diversities, - neutrality stats like
Tajima's D, and - population divergence stats like Fst. Each of these
stats depends on the SFS -- the Site Frequency Spectrum So, our workflow
will be to use the .saf files to estimate the SFS, and then use the SFS
to estimate our diversity stats and Fst.

-   Create ANGSD_doTheta.sh to estimate the SFS and nucleotide diversity
    stat
-   Based on the saf.idx files from ANGSD GL calls, first estimate the
    Site Frequency Spectrum (SFS)

```         
~/myscripts
vim ANGSD_doTheta.sh

REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

OUT=~/myresults/ANGSD

MYPOP=""

SUFFIX=""

###Estimation of the SFS for all sites using the FOLDED SFS
realSFS ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -maxIter 1000 \
        -tole 1e-6 \
        -P 1 \
        -fold 1 \
        > ${OUT}/${MYPOP}_${SUFFIX}.sfs
        
###After the SFS, estimate the theta diversity stats
Once you have the SFS: estimate the thetas and neutrality stats by adding the following code chunk at the end of your ANGSD_doTheta.sh script:

# Estimate thetas and stats using the SFS

realSFS saf2theta ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -sfs ${OUT}/${MYPOP}_${SUFFIX}.sfs \
        -outname ${OUT}/${MYPOP}_${SUFFIX}

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx
        
###If we wanted to analyze this on sliding windows, we could instead replace the above code chunk with the following:

# For sliding window analysis:

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx \
-win 50000 \
-step 10000 \
-outnames ${OUT}/${MYPOP}_${SUFFIX}.thetasWindow.gz
```

The unfolded vs. folded SFS:

The big difference here is whether we are confident in the ancestral
state of each variable site (SNP) in our dataset:

-   If we know the ancestral state, then the best info is contained in
    the unfolded SFS, which shows the frequency histogram of how many
    derived loci are rare vs. common -- bins in the unfolded SFS go from
    0 to 2N -- why?

-   When you don't know the ancestral state confidently, you can make
    the SFS based on the minor allele (the less frequent allele; always
    \< 0.5 in the population). -- bins in the folded SFS go from 0 to 1N
    -- why?

Essentially, the folded spectra wraps the SFS around such that high
frequency "derived" alleles are put in the small bins (low minor allele
freq).

*.thetas.idx.pestPG For either of the results files above, the first
column of the results file (*.thetas.idx.pestPG) is formatted a bit
funny and we don't really need it. There are also a bunch of other
neutrality stats that we don't need right now. We can use the cut
command to get rid of these extra columns. The below cut command retains
columns 2-5, 9 and 14

```         
cut -f2-5,9,14 ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx.pestPG > ${OUT}/${MYPOP}_${SUFFIX}.thetas
```

The columns correspond to the following stats:

Col Statistic Description 2 Chr The chromosome or (more appropriately)
contig being analyzed 3 WinCenter The center of the contig, in bp 4 tW
Watterson's Theta -- an estimate of nucleotide diversity based on
segregating sites 5 tP Theta Pi -- estimate of nucleotide diversity
based on pairwise divergence 9 Tajima Tajima's D -- a neutrality stat
that tests for the difference in tW-tP 14 nSites The number of base
pairs being analyzed along this stretch of cont.

### Summarize diversity stats in R

-   use Filezilla to download these 2 files: .thetas.idx.pestPG .sfs
    Save to results folder in githuib repo so we can import into R to
    look at the mean and variability in nucleotide diversity for our pop

RStudio

```         
setwd("") # set your path to your results folder in your repo where you saved your diversity stats file

list.files() # list out the files in this folder to make sure you're in the right spot.

# First let's read in the diversity stats
theta <- read.table("_.thetas",sep="\t",header=T)

theta$tWsite = theta$tW/theta$nSites #scales the theta-W by the number of sites
theta$tPsite = theta$tP/theta$nSites #scales the theta-Pi by the number of sites

summary(theta)

# You can order the contig list to show you the contigs with the highest values of Tajima's D, or the lowest

head(theta[order(theta$Tajima, decreasing = TRUE),]) # top 10 Tajima's D values

head(theta[order(theta$Tajima, decreasing = FALSE),]) # bottom 10 Tajima's D values

#You can also look for contigs that have combinations of high Tajima's D and low diversity -- these could represent outliers for selection
#theta[which(theta$Tajima>1.5 & theta$tPsite<0.001),]


sfs<-scan('9999_.sfs')
sfs<-sfs[-c(1,which(sfs==0))]
sfs<-sfs/sum(sfs)

# Be sure to replace "9999" with your pop code in the "main" legend below
barplot(sfs,xlab="Chromosomes",
        names=1:length(sfs),
        ylab="Proportions",
        main="Pop 9999 Site Frequency Spectrum",
        col='blue')

# Put the nucleotide diversities, Tajima's D, and SFS into a 4-panel figure
par(mfrow=c(2,2))
hist(theta$tWsite, xlab="theta-W", main="Watterson's theta")
hist(theta$tPsite, xlab="theta-Pi", main="Pairwise Nucleotide Diversity")
hist(theta$Tajima, xlab="D", main="Tajima's D")
barplot(sfs,names=1:length(sfs),main='Site Frequency Spectrum')


# To reset the panel plotting, execute the line below:
dev.off()
```

### Use ANGSD and the SFS for multiple pops to calculate genetic divergence between pops (Fst)

-   Calculate Fst between any pair of populations by comparing their SFS
    to each other. For this, we'll need to estimate the SFS for pairs of
    populations; we can each contribute to the overall analysis by
    looking at how our focal pop is divergent from the others.

-   calculate Fst between our focal red spruce population (MYPOP) and
    the black spruce samples (tell us which of our pops might be
    hybridizing). What would we expect for Fst in this case?

Write a bash script called ANGSD_Fst.sh that includes the following
code:

```         
# Start with the usual bash script header
# Give yourself some notes
# Path to Black Spruce (BS) input saf.idx data:
BLKSPR="/netfiles/ecogen/PopulationGenomics/fastq/black_spruce/cleanreads/bam/ANGSD"
#Path to save your output:
OUTPUT=

MYPOP=""

SUFFIX=""

# Estimate Fst between my red spruce pop and black spruce:

realSFS ${MYPOP}_.saf.idx ${BLKSPR}/BS_all.saf.idx -P 1 >${MYPOP}_BS.sfs
realSFS fst index ${MYPOP}_.saf.idx ${BLKSPR}/BS_all.saf.idx -sfs ${MYPOP}_BS.sfs -fstout ${MYPOP}_BS -whichFst 1
realSFS fst stats ${MYPOP}_BS.fst.idx 
```

### *pcANGSD.* Visualize popualtion structure across the landscape using PCA and Admixture

-   Visualize differences in the genetic structure or genetic ancestry
    in our sample: approach this using PCA or Admixture analysis. We can
    do each of these approaches on genotype likelihoods in *ANGSD* using
    a special routine called *pcANGSD.*

*pcANGSD* :refines the estimation of allele frequencies for each
individual at the same time that it finds the clusters that individual
may have ancestry within. Since we want to run pcANGSD for the entire
set of samples -- not just your focal pop -- we need the genotype
likelihoods from ANGSD for all 95 red spruce samples. That would take a
long time to run (about 24 hrs) and would be redundant for each of you
to do, so I ran these once for the class.

For your reference (and future work), the code I used to estimate the
genotype likelihoods is here: (you don't have to run this now!)

```         
#/netfiles/ecogen/PopulationGenomics/scripts/ANGSD_allRS_poly.sh

#and exported the genotype likelihoods in “beagle” format here:

#/netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.beagle.gz

We can use the beagle file containing the genotype likelihoods for all 95 red spruce samples as input to pcANGSD. The script is actually not too bad…let’s give it a go:

# Start with the usual bash script header

# Give yourself some notes

# Path to your input data (where the beagle file lives)

INPUT=

# Path to save your output (in your home directory):

OUTPUT=

SUFFIX="allRS_poly"

# Make a copy of the list of bam files for all the red spruce samples and place in your home directory. You'll need this later for making figures.

cp ${INPUT}/allRS_bam.list ${OUTPUT}


# To run pcANGSD, you need to activate a "virtual environment" on the server by including the line below:

source /data/popgen/pcangsd/venv/bin/activate  


# Then, run PCA and admixture scores with pcangsd:

pcangsd -b ${INPUT}/${SUFFIX}.beagle.gz \
        -o ${OUTPUT}/${SUFFIX} \
        -e 1 \
        --admix \
        --admix_alpha 50 \
        --threads 1 
```

-   This will run pcANGSD assuming it fits a single "eigenvalue" to
    split your samples into K=2 clusters. If you want to explore higher
    levels of clustering in the future, you can include the -e <int>
    flag, where is a number that is K-1 number of clusters you want to
    fit.

-   Once the run is finished, use FileZilla to transfer the following
    files over to your repo on your laptop: allRS_bam.list
    allRS_poly.cov allRS_poly.admix.2.Q

RStudio and let's start making some figures!

```         
library(ggplot2) # plotting
library(ggpubr) # plotting

setwd("") # set the path to where you saved the pcANGSD results on your laptop

## First, let's work on the genetic PCA:

COV <- as.matrix(read.table("allRS_poly.cov")) # read in the genetic covariance matrix

PCA <- eigen(COV) # extract the principal components from the COV matrix

## How much variance is explained by the first few PCs?

var <- round(PCA$values/sum(PCA$values),3)

var[1:3]

# A "screeplot" of the eigenvalues of the PCA:

barplot(var, 
        xlab="Eigenvalues of the PCA", 
        ylab="Proportion of variance explained")

## Bring in the bam.list file and extract the sample info:

names <- read.table("allRS_bam.list")
names <- unlist(strsplit(basename(as.character(names[,1])), split = ".sorted.rmdup.bam"))
split = strsplit(names, "_")
pops <- data.frame(names[1:95], do.call(rbind, split[1:95]))
names(pops) = c("Ind", "Pop", "Row", "Col")

## A quick and humble PCA plot:

plot(PCA$vectors[,1:2],
     col=as.factor(pops[,2]),
     xlab="PC1",ylab="PC2", 
     main="Genetic PCA")
     
## A more beautiful PCA plot using ggplot :)

data=as.data.frame(PCA$vectors)
data=data[,c(1:3)]
data= cbind(data, pops)

cols=c("#377eB8","#EE9B00","#0A9396","#94D2BD","#FFCB69","#005f73","#E26D5C","#AE2012", "#6d597a", "#7EA16B","#d4e09b", "gray70")

ggscatter(data, x = "V1", y = "V2",
          color = "Pop",
          mean.point = TRUE,
          star.plot = TRUE) +
  theme_bw(base_size = 13, base_family = "Times") +
  theme(panel.background = element_blank(), 
        legend.background = element_blank(), 
        panel.grid = element_blank(), 
        plot.background = element_blank(), 
        legend.text=element_text(size=rel(.7)), 
        axis.text = element_text(size=13), 
        legend.position = "bottom") +
  labs(x = paste0("PC1: (",var[1]*100,"%)"), y = paste0("PC2: (",var[2]*100,"%)")) +
  scale_color_manual(values=c(cols), name="Source population") +
  guides(colour = guide_legend(nrow = 2))

## Next, we can look at the admixture clustering:

# import the ancestry scores (these are the .Q files)

q <- read.table("allRS_poly.admix.2.Q", sep=" ", header=F)

K=dim(q)[2] #Find the level of K modeled

## order according to population code
ord<-order(pops[,2])

# make the plot:
barplot(t(q)[,ord],
        col=cols[1:K],
        space=0,border=NA,
        xlab="Populations",ylab="Admixture proportions",
        main=paste0("Red spruce K=",K))
text(tapply(1:nrow(pops),pops[ord,2],mean),-0.05,unique(pops[ord,2]),xpd=T)
abline(v=cumsum(sapply(unique(pops[ord,2]),function(x){sum(pops[ord,2]==x)})),col=1,lwd=1.2)
```

### How has selection acted to drive divergence of individual loci in excess of the population structure we've observed?

-   When selection acts in response to local environmental conditions:
    we observe an excess of population structure at certain loci.

-   This can be thought of as Fst at a single locus exceeding some
    background level of divergence that exists across the genome as a
    whole.

-   We call these *"Fst outliers"* and identifying these outliers is a
    major goal in ecological genomic studies.

-   But, a major challenge is how best to control for background
    population structure -- in other words, how can we sift out the
    "outliers" from the rest of the population structure in the genome?

-   One approach to identifying Fst outliers is using genetic PCA to:

(a) identify the major axes of population structure, and then
(b) find loci that "load" very strongly on these axes, indicating their
    exceptional divergence is probably driven by the action of selection
    in excess of genetic drift.

-   Run a scan for Fst outliers using *pcANGSD*, just like we did for
    the genetic PCA, but here we initiate a selection scan for loci that
    are exceptionally divergent along one or more of the inferred
    genetic PC axes.
-   This method is especially helpful when it's hard to define what are
    genetic "populations", since it uses the genetic PCA to infer the
    genetic structure directly. The method employed in pcANGSD follows
    the *"FastPCA"* selection scan method described by Galinsky et al.
    2015
-   The approach essentially uses the SNP weights or "loadings" on a
    given genetic PC axis to determine the strength of association, and
    if it exceeds the expectation due to neutral drift.

### pcANGSD_selection.sh

-   Write a bash script called pcANGSD_selection.sh that includes the
    following code. Recall that pcANGSD uses genotype likelihoods in
    "beagle" format, can be found here:
    /netfiles/ecogen/PopulationGenomics/ANGSD/

```         
cd ~ /myresults/ANGSD
vim pcANGSD_selection.sh
INPUT=""

OUTPUT=

SUFFIX="allRS_poly"

cp ${INPUT}/allRS_bam.list ${OUTPUT}

# Activates the pcANGSD environment and run the selection scan:

source /data/popgen/pcangsd/venv/bin/activate
        
pcangsd -b ${INPUT}/${SUFFIX}.beagle.gz \
    -o ${OUTPUT}/${SUFFIX} \
    -e <what number should we use here?>
    --selection \
    --minMaf 0.05 \
    --sites_save \
    --snp_weights \
    --threads 1
#Note: _minMaf 0.05_ option tests only those loci that have a minor allele frequency of 0.05 or greater. That’s because there’s little statistical power for testing association of alleles that are very rare (<5% in the sample).    
```

a new set of files, including: File name contents
allRS_poly.selection.npy selection scores for each locus on each tested
PC axis allRS_poly.weights.npy weights that show how strongly each locus
"loads" on each PC axis allRS_poly.sites for each locus shows whether it
got tested (1) or not (0)

-   Get a file with minor allele frequencies ("maf") that ANGSD makes
    when first estimating genotype likelihoods. You have one of these
    files from when you worked on your focal pops, but we need one for
    the "allRS_poly" run of all red spruce individuals for just
    polymorphic sites.

```         
/netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.mafs.gz
```

We need to combine the *"mafs"* file and the\_ "sites" \_file to create
a file that has the contig and position info for each locus that got
tested for selection.

```         
#Here’s a few lines of bash code to create this file:
#First, we need to add a header to the “sites” file:

sed -i '1i kept_sites' ~/myresults/ANGSD/allRS_poly.sites

#Next, we need to get the contig and position info out of the “mafs” file and combine it with the “sites” file:

zcat /netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.mafs.gz | cut -f1-7 | paste - ~/myresults/ANGSD/allRS_poly.sites >allRS_poly_mafs.sites

#Transfer the 3 listed above, plus the new allRS_poly_mafs.sites, using FileZilla and save into your repo on your laptop. 
```

### Idenitfy and visualize outlier loci in R

Use R to: - import the selection scan results and associated
meta-data, - assign p-values for each tested locus, - and visualize the
results!

```         
###################################
#  Selection scans for red spruce #
###################################

library(RcppCNPy) # for reading python numpy (.npy) files

setwd("~/Documents/Github/Ecological_Genomics/Fall_2023/pcangsd/")

list.files()

###read in selection statistics (these are chi^2 distributed)

s<-npyLoad("allRS_poly.selection.npy")

#convert test statistic to p-value
pval <- as.data.frame(1-pchisq(s,1))
names(pval) = "p_PC1"

##read positions
p <- read.table("allRS_poly_mafs.sites",sep="\t",header=T, stringsAsFactors=T)
dim(p)

p_filtered = p[which(p$kept_sites==1),]
dim(p_filtered)

#How many sites got filtered out when testing for selection? Why?

##make manhattan plot
plot(-log10(pval$p_PC1),
  col=p_filtered$chromo,
  xlab="Position",
  ylab="-log10(p-value)",
  main="Selection outliers: pcANGSD e=1 (K2)")

#We can zoom in if there's something interesting near a position...

plot(-log10(pval$p_PC1[2e05:2.01e05]),
  col=p_filtered$chromo, 
  xlab="Position", 
  ylab="-log10(p-value)", 
  main="Selection outliers: pcANGSD e=1 (K2)")

#get the contig with the lowest p-value for selection
sel_contig <- p_filtered[which(pval==min(pval$p_PC1)),c("chromo","position")]
sel_contig

#get all the outliers with p-values below some cutoff
cutoff=1e-3   # equals a 1 in 5,000 probability
outlier_contigs <- p_filtered[which(pval<cutoff),c("chromo","position")]
outlier_contigs

#how many outlier loci < the cutoff?
dim(outlier_contigs)[1]

#how many unique contigs harbor outlier loci?
length(unique(outlier_contigs$chromo))

#How do we find out what functional genes are contained on the unique contigs harboring outliers?
#use the Picea abies reference genome annotation to get the genes based on the outlier contigs, then test for enrichment of gene function using available public databases.

#First, export your unique outlier contigs in R:

write.table(unique(outlier_contigs$chromo),
  "allRS_poly_PC1_outlier_contigs.txt", 
  sep="\t",
  quote=F,
  row.names=F,
  col.names=F)
```

-   Transfer back to the server

-   Grep out the gene IDs. using a series of piped commands like so:

-   Define your paths to the ref genome annotation on the server and
    your newly made outlier loci file

-   Use zcat to open the annotation without unzipping it, then pipe to
    grep and use the -f flag to take the contig names from the outliers
    file -- this is a handy trick, and saves us a lot of time from
    having to manually input each contig one at a time and search!

-   Pipe to a new grep to get just the contigs containing genes

-   Pipe the list of gene IDs to take just the unique ones, cut out the
    9th column (containing the gene IDs), and get rid of the annoying
    "ID=" portion of each gene ID

```         
ANNOT="/netfiles/ecogen/PopulationGenomics/ref_genome/annotation/Pabies1.0-all-cds.gff3.gz"

OUTLIERS=~/myresults/ANGSD/allRS_poly_PC1_outlier_contigs.txt

zcat ${ANNOT} | grep -f ${OUTLIERS} | grep "gene" | uniq | cut -f9 | sed "s/ID=//g"
```

### *Plantgenie*

You should now have a list of gene IDs printed to your screen. You can
either copy these by highlighting with your mouse and clicking or you
can save to an external file for later (using the \>outfile.txt command)

-   Take your gene IDs and go to the *plantgenie website*.
-   create a gene list using your copied IDs, determine which genes they
    correspond to (not all wil be annotated!), and test for functional
    enrichment:
-   In the upper left corner, see a circle with 3 red bars --\> click
    it, and create a new gene list
-   Paste your gene IDs into the list, and plantgenie will search the P.
    abies annotation and return all the known genes
-   Click the "+" button to save these to the current gene list, then
    close out
-   Go to the "Analysis Tools" tab on the top bar of the page, and
    choose "Enrichment"
-   *Plantgenie* will now test for whether the annotation Gene Ontology
    (GO) and Protein Family (Pfam) functional descriptions assigned to
    each gene ID are over-represented compared to all other genes
    containing those GO and Pfam categories in the P. abies genome.

Relate back to the biology!

What sort of functional genes did you find are represented in your
outliers? Are any enriched for certain GO or Pfam functions? (focus on
p-values \<\< 0.01, or q-values \< 0.05) Remember the population
structure axis (genetic PC) these loci are outliers along...are any of
the enriched functions interesting in light of this structure? Any
surprising?

### Genotype-Environment Association (GEA)

-   We've now learned about outlier loci that show extreme population
    structure along genetic PC axes.
-   These are loci that tend to have high Fst (population structure)
    suggestive of selection and local adaptation.
-   How does this relate to different aspects of the environment? How do
    we know what environmental gradients might be responsible for
    driving this population differentiation? Where's the "landscape" in
    all this genomics stuff?

-- GEA comes in -- short for "Genotype-Environment Association". ---The
whole idea behind GEA is to look for loci that show strong associations
between allele frequencies and environmental gradients, like climate.
----There's a substantial literature on GEA methods in landscape
genomics, but a good background paper is Rellstab et al. (2015).

For any GEA analysis, you essentially need two things: - Genotype data
from individuals sampled across one or more environmental gradients. \_
Environmental data (like cimate) you want to test as drivers of the
selection on allele frequencies.

### *Bioclim* climate data

-   use R to query the *Worldclim* climate database and pull out the
    bioclim variables for each of our samples based on their lat/longs.

```         
# You made need to use "install.packages" if you don't have some of the below libraries already

library(raster)
library(FactoMineR)
library(factoextra)
library(corrplot)

setwd("")

bio <- getData("worldclim",var="bio",res=10)

coords <- read.csv("https://www.uvm.edu/~kellrlab/forClass/colebrookSampleMetaData.csv", header=T)

The chunk below refers to your bamlist file that you transferred during last week's PCA/admixture analysis.  It should be the same one you want to use here -- if your sample list for analysis changes in the future, you'll need a different bamlist!

names <- read.table("pcangsd/allRS_bam.list")
names <- unlist(strsplit(basename(as.character(names[,1])), split = ".sorted.rmdup.bam"))
split = strsplit(names, "_")
pops <- data.frame(names[1:95], do.call(rbind, split[1:95]))
names(pops) = c("Ind", "Pop", "Row", "Col")

angsd_coords <- merge(pops, coords, by.x="Ind", by.y="Tree")

points <- SpatialPoints(angsd_coords[c("Longitude","Latitude")])

clim <- extract(bio,points)

angsd_coords_clim <- cbind.data.frame(angsd_coords,clim)
str(angsd_coords_clim)
```

-   don't want to test all 19 variables in our GEA, but rather just one
    or a few that seem to capture the climate gradients in our samples.
-   For this, we can use PCA on the climate data (so, an environmental,
    not a genetic PCA in this case).
-   Using the climate PCA can then let us see which variables most
    strongly define the climate space of red spruce, and we can then use
    these for our GEA test.

```         
clim_PCA = PCA(angsd_coords_clim[,15:33], graph=F)

# screeplot of PCA eigenvalues
fviz_eig(clim_PCA)

# plot the variable loadings on the 1st 2 PCs
fviz_pca_var(clim_PCA)

# Which variables show the strongest loading on each axis?
var <- get_pca_var(clim_PCA)
corrplot(var$cos2, is.corr=FALSE)
  
# We can ask the same question using correlation tests of each variable with each PC axis:
dimdesc(clim_PCA)

# Lastly, how do our samples fall out in climate PCA space?
fviz_pca_ind(clim_PCA, 
             geom.ind="point",
             col.ind = angsd_coords_clim$Latitude, 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             legend.title="Latitude")
```

```         
#pick 1 Bioclim variabe for each PC1 and 2 axis and export the values:
write.table(scale(angsd_coords_clim[,c("bioXX","bioYY")]),
            "allRS_bioXX_YY.txt",
            sep="\t",
            quote=F,
            row.names = F,
            col.names=F)
```

-   Use FileZilla to transfer it to the server and into your
    \~/myresults/ANGSD folder. \_ Use the cut command to write out
    separate environmental files for each variable (ANGSD only tests one
    variable at a time).

Can you think of a cut statement to take your allRS_bioXX_YY.txt file as
input and write out a new output called allRS_bioXX.txt that has just
the first variable in it? Then try it again for the 2nd variable.

```         
cut options someinfile >someoutfile
```

### Getting our GEA run on the server started! *AGNSD* *doAsso*

-   The GEA analysis will likely take some time -- maybe over a day to
    run.
-   We'll use ANGSD again to test for associations between allele
    frequencies and the bioclim variables while using genotype
    likelihoods to account for uncertainty.
-   The ANGSD routine we'll use is called *doAsso* which stands for *"do
    Association"*. It has it's own manual page on ANGSD's website
-   *AGNSD* *doAsso* requires genotype probabilities (yes, these are
    slightly different mathematical than genotype likelihoods), which
    means we'll need to call the initial set of ANGSD commands to import
    the bam's and filter them before sending the probabilities to the
    doAsso function.

```         
bash header

notes


REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

SUFFIX=""

INPUT=""

OUTPUT=

# Make a list of all the sorted and PCR dup removed bam files:

ls /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/2*sorted.rmdup.bam > ${OUTPUT}/allRS_bam.list

# Run ANGSD to estimate the Geno Probs and then perform the GEA using doAsso:

ANGSD -b ${OUTPUT}/allRS_bam.list \
-ref ${REF} -anc ${REF} \
-out ${OUTPUT}/${SUFFIX} \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-GL 1 \
-doSaf 1 \
-doCounts 1 \
-minInd 47 \
-setMinDepthInd 1 \
-setMaxDepthInd 40 \
-skipTriallelic 1 \
-doMajorMinor 1 \
-doMaf 1 \
-SNP_pval 1e-6 \
-minMaf 0.05 \
-doPost 1 \
-doAsso 5 \
-yQuant ${OUTPUT}/allRS_bioXX.txt
```

That's a lot of options for ANGSD! Here's the breakdown (after the
obvious in/out and reference genome at the beginning):

Code option Meaning -nthreads how many CPU's you want to use
-remove_bads discards reads that don't map as a pair to a unique
location in the ref genome\
-C 50 downgrades the quality of reads with excessive mismatches to the
ref genome -baq 1 performs re-calibration of base Q-scores around
regions of structural variation\
-minMapQ 20 discards reads with mapping Q-score \<20 -minQ 20 sets bases
with Q-scores \< 20 to missing -GL 1 estimate genotype likelihoods using
the Samtools algorithm -doSAF 1 estimate site-allele frequencies
-doCounts 1 count mapped bases at each site -- needed for filters below
-minInd 47 skip loci that have \< minInd number of individuals with
data; set to \~50% of your full sample size -setMinDepthInd 1 set
individuals with \<1 read per site to missing for that site
-setMaxDepthInd 40 set individuals with \> 40 reads per site to missing
for that site -skipTriallelic 1 if there are \>2 alleles present at a
site -- discard! -doMajorMinor 1 calculate which allele is the most
common (major) and which is rare (minor) -doMaf 1 compute the minor
allele frequencies -SNP_pval 1e-6 discard sites unlikely to be
polymorphic, based on a p-value of \<1e-6 -minMaf 0.05 discard sites
with minor allele frequencies \< 5%

-   Most of these are identical to what I used for calling the genotype
    likelihoods (beagle files) that we used as input into pcANGSD.
-   see also the ANGSD manual online so you could have them in case you
    need to change anything.

Those last 3 lines are the important part for the GEA:

Code option Meaning doPost 1 compute genotype probabilities doAsso 5
perform the GEA association test -yQuant path and filename containing
the input environmental variable to test

-   You'll also notice that we do this just on loci that are present in
    at least 5% frequeny across the entire set of samples (-minMaf 0.05)

```         
#Save your script as ANGSD_GEA.sh
tmux
bash
dettach from screen 
top #to see if you’re running!
```
The results will be stored in a file called: allRS_poly_bioXX.lrt0.gz where “bioXX is the climate variable you supplied to ${BIOVAR} in the script.

```
#peek inside this file:
zcat allRS_poly_bio10.lrt0.gz | head
```
Inside the file:
Column	            Meaning
LRTscore	        Likelihood Ratio Test (LRT) of whether the SNP is associated with climate
high_WT/HE/HO	    The counts of homozygous (wildtype):heterozygous:homozygous genotypes
LRTem	            Another version of the likelihood ratio test (this is the one used to compute significance)
beta	            The slope of minor allele frequencies along the climate gradient
SE                	The standard error of the slope
emIter            	Number of iterations of the EM algorithm used to test for significance in the GEA

- Most of the head on this file has lots of ‘nan’. That means those SNPs were not significant (i.e., had no significant association between allele frequencies and the climate gradient).

```
#Quickly find the loci that ARE signficant by doing an “inverted” grep search that finds all the lines in the file that DON’T have a match to your search term. 

zcat allRS_poly_bio10.lrt0.gz | grep -v "nan" | head
```
### 
