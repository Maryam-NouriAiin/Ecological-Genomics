---
title: Pop Gen Lectures
author: Maryam Nouri-Aiin
date: Sep 11, 2023
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---


# Table of Contents:   
* [Entry 1: 2023-09-11](#id-section1)
* [Entry 2: 2023-09-13](#id-section2)
* [Entry 3: 2023-09-18](#id-section3)
* [Entry 4: 2023-09-20](#id-section4)
* [Entry 5: 2023-09-25](#id-section5)
* [Entry 6: 2023-09-27](#id-section6)
* [Entry 7: 2023-10-02](#id-section7)
* [Entry 8: 2023-10-04](#id-section8)
* [Entry 9: 2023-10-09](#id-section9)
* [Entry 10: 2023-10-11](#id-section10)
* [Entry 11: 2023-10-16](#id-section11)
* [Entry 12: 2023-10-18](#id-section12)
* [Entry 13: 2023-10-23](#id-section13)
* [Entry 14: 2023-10-26](#id-section14)
* [Entry 15: 2023-11-30](#id-section15)
* [Entry 16: 2023-11-01](#id-section16)
* [Entry 17: 2023-11-06](#id-section17)
* [Entry 18: 2023-11-08](#id-section18)
* [Entry 19: 2023-11-13](#id-section19)
* [Entry 20: 2023-11-15](#id-section20)
* [Entry 21: 2023-11-20](#id-section21)
* [Entry 22: 2023-11-22](#id-section22)
* [Entry 23: 2023-11-27](#id-section23)
* [Entry 24: 2023-11-29](#id-section24)
* [Entry 25: 2023-12-04](#id-section25)
* [Entry 26: 2023-12-06](#id-section26)

<div id="id-section1"></div>


## Entry 1: 2023-09-11.

### Ecological Genomics Tutorials: Population & Landscape Genomics 1
September 11, 2023


### 2. Here’s our “pipeline”
- Visualize the quality of raw data (Program: FastQC)
- Clean raw data (Program: Trimmomatic)
- Visualize the quality of cleaned data (Program: FastQC)
- Calculate #’s of cleaned, high quality reads going into mapping

##### Then use these cleaned reads to align to the reference genome next time so that we can start estimating genomic diversity and population structure.

### 3.-5. Visualize, Clean, and Visualize again

###### Whenever you get a new batch of NGS data, the first step is to look at the data quality of coming off the sequencer and see if we notice any problems with base quality, sequence length, PCR duplicates, or adapter contamination. A lot of this info is stored in the raw data files you get from the core lab after sequencing, which are in “fastq” format.

### The fastq files for our project are stored in this path: /netfiles/ecogen/PopulationGenomics/fastq/red_spruce

***cd over there and ll to see the files.***

###### There should be 190 fastq files – 2 for each of the 95 samples (2 files/sample because these are paired-end reads, and each sample gets a file with the forward reads (R1) and another with the reverse reads (R2)).

###### The naming convention for our data is: <PopCode>_<RowID>_<ColumnID>_<ReadDirection>.fast.gz

##### Together, <PopCode>_<RowID>_<ColumnID> define the unique individual ID for each DNA sample, and there should be 2 files per sample (and R1 and an R2)

### So…what is a ***.fastq file*** anyway?
***A fastq file is the standard sequence data format for NGS. It contains the sequence of the read itself, the corresponding quality scores for each base, and some meta-data about the read.***


###### The files are big (typically many Gb compressed), so we can’t open them completely. Instead, we can peek inside the file using head. But _size these files are compressed (note the **.gz** ending in the filenames)_, and we want them to stay compressed while we peek. Bash has a solution to that called **zcat**. This lets us look at the .gz file without uncompressing it all the way. Let’s peek inside a file:


### zcat 2505_9_C_R2.fastq.gz | head -n 4
@A00354:455:HYG3FDSXY:1:1101:3893:1031 2:N:0:CATCAAGT+TACTCCTT
GTGGAAAATCAAAACCCTAATGCTGAAAGGAATCCAAATCAAATAAATATTTTCACCGACCTGTTTCGATGCCAGAATTGTCTGCGCAGAAGACTCGTGAAATTTCGCCAGCAGGTAAAATTAAAAGGCTAGAATTAACCGCTGAAATGGA
+
FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:FFFF:FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF:F:F
Note: zcat lets us open a .gz (gzipped) file; we then “pipe” | this output from zcat to the head command and print just the top 4 lines -n4

### The fastq file format has 4 lines for each read:

#### Line	Description
#### 1	Always begins with ‘@’ and then information about the read
#### 2	The actual DNA sequence
#### 3	Always begins with a ‘+’ and sometimes the same info in line 1
#### 4	A string of characters which represent the quality scores; always has same number of characters as line 2
Here’s a useful reference for understanding Quality (Phred) scores. If P is the probability that a base call is an error, then:
Q = -10*log10(P)

So:
Phred Quality Score	Probability of incorrect base call	Base call accuracy
10	1 in 10	90%
20	1 in 100	99%
30	1 in 1000	99.9%
40	1 in 10,000	99.99%
The Phred Q score is translated to ASCII characters so that a two digit number can be represented by a single character.

### Quality encoding: !"#$%&'()*+,-./0123456789:;<=>?@ABCDEFGHI
                   |         |         |         |         |
    Quality score: 0........10........20........30........40   
What kind of characters do you want to see in your quality score?

### Visualize using FastQC

#### We’re going to use the program ***FastQC*** (already installed on our server). ***FastQC looks at the quality collectively across all reads in a sample.***
First, let’s _cd back to our home directories ~/_ and **set up some new folders** to store our work. We’ll make 3 directories to store our data, scripts, and results:

mkdir mydata/

mkdir myscripts/

mkdir myresults/

##### Then let’s cd into the myresults/ folder then use pwd to prove to yourself that you’re in the myresults/ folder within your home directory. It should look like this (but with your home directory info instead of mine):

[kellrlab@ecogen myresults]$ pwd

/users/k/e/kellrlab/myresults

#### Now within myresults/ 

##### let’s make another folder called fastqc/ to hold the outputs from our QC analysis. Do that on your own, just like we did above, then cd into the fastqc/ folder and type pwd again to prove to yourself you did it right.

#### Now, we’re ready to run FastQC to look at the quality of our sequencing. The basic command is like this:

### fastqc filename.fastq.gz -o outputdirectory/

#### This will generate an .html output file for each input file you’ve run.

###### Once you’ve got results, let’s use ***Filezilla to transfer the folder ~/myresults/fastqc/ over to your laptop and into the results folder in your github repo***. Once the file transfer is complete, go to where you saved your ***files on your laptop and try double-clicking one of the html outputs***. It should open with a web browser.

###### How does the quality look?

###### Since we made some changes (added files) to our github repo, we should practice committing these and then pushing to GitHub!


------   

<div id="id-section2"></div>


## Entry 2: 2023-09-13.
### Ecological Genomics Tutorials: Population & Landscape Genomics 2
September 13, 2023

### Learning Objectives for 09/13/23

#### Review visualizing QC on our sequence reads
#### Trim reads using fastp and assess pre- and post trimming
#### Start mapping (a.k.a. aligning) each set of cleaned reads to a reference genome
#### Visualize sequence alignment files
#### Calculate mapping statistics to assess quality of the result


### Clean using fastp

##### Now that we have a sense of the quality of our data, and where we might need some trimming, our next step is to clean the reads by removing low quality bases and leftover Illumina sequence adapters from the sequences.

#### We’ll use the ***fastp program*** to : . See also the paper published by Chen in May 2023. Another very similar **program for trimming is Trimmomatic**. It works great, but fastp does everything Trimmomatic does but in only a faction of the time! The program is already installed on our server:

## /data/popgen/fastp

###### This job will be bit more complicated, since we want you each to process all the samples for a given population.

##### Since we’re doing multiple samples from the same population, we want to be clever and process in a batch instead of manually one at a time. We can do this by writing a bash script that contains a **loop**. This is the first taste of how bash scripting is powerful!

The basic syntax of a bash loop is like this:


cd <path to the input data>


for FILE in somelist

do

  command1 -options ${FILE} -moreOptions
  command2 -options ${FILE} -moreOptions

done

#### Note the use of variable assignment using ${}. 

#### We define the word FILE in the “for loop” as the variable of interest, and then call the iterations of it using ${FILE}. For example, we could use the wildcard character (*) in a loop to call all files that include the ID code for your population and then pass those filenames in a loop to the commands. For example, if your pop was “2XXX”, then you could write a loop that would process all the R1 fastq files in that population like this:

MYPOP="2XXX"

cd <path to the input data>

for FILE in ${MYPOP}*R1.fastq.gz

do

  command1 -options ${FILE} -moreOptions
  command2 -options ${FILE} -moreOptions

done

###### That’s the basic idea! Let’s do it…

We’ve provided a partially completed example script here:

/netfiles/ecogen/PopulationGenomics/scripts/fastp.sh

1. Make a copy of the script cp and put it into your ~/myscripts directory
2. Open and edit your copied script using the program vim vim ~/myscripts/fastp.sh.
3. Edit the file so that you’re trimming the fastq files for the population code assigned to you; you can add annotations as notes to yourself using the hashtag (#)
4. Save the file after you’re done making changes. To do this, hit to get out of “Insert” mode, then :wq This will write (w) the changes to your file and then quit (q) vim.
NOTE: fastp needs both read pairs (i.e., the R1 and R2 files) given to it at the same time. This makes the use of name variables in the loop a bit tougher since we can’t rely just on the population name (example, 2505) but also need each individual name too (example, 2505_9_C).

### We’ll use a couple of tricks to recycle the R1 file name to quickly create a mathcing R2 file name within the loop. We’ll do a simialr trick to quickly name the output files adding “_clean” to the end. We’ll talk about this in class together, but there are also annotations in the script itself explaining what’s happening.

##### Once you’re ready, go ahead and 

that you saved into your ~/myscripts/ folder:

cd ~/myscripts/
bash fastp.sh
Note that the output (the trimmed and cleaned reads) are going to get saved to a folder on the network drive that you’ve been given write access to: /netfiles/ecogen/PopulationGenomics/fastq_red_spruce/cleanreads

## Visualize pre- and post-trimming read quality with the fastp html output

Conveniently, fastp is not only fast, but also produces a summary of the change in quality pre- and post-trimming. Just like FastQC, the output is in an html file that we can use FileZilla to transfer back to our laptops and look at in a browser. Pretty cool!

Mapping cleaned and trimmed reads against the reference genome
Now that we have cleaned and trimmed read pairs, we’re ready to map them against the reference genome.

***The first step of read mapping is downloading the reference genome, freely available from the developers at plantgenie.org: The Picea abies reference genome is based on Norway spruce (P. abies) and published by Nystedt et al. (2013).***



#### You don’t actually need to download the genome because we already have the file in the directory below. But for future reference ***wget is a useful command to ownload files from the web***.

cd /netfiles/ecogen/PopulationGenomics/ref_genome
wget "ftp://plantgenie.org:980/Data/PlantGenIE/Picea_abies/v1.0/fasta/GenomeAssemblies/Pabies01-genome.fa.gz"

#### Rather than trying to map to the entire 19.6 Gbp reference (yikes!), we first subsetted the P. abies reference to include just the contigs that contain one or more probes from our exome capture experiment. For this, we did a BLAST search of each probe against the P. abies reference genome, and then retained all scaffolds that had a best hit.

This reduced reference contains:
1,376,182,454 bp (~1.38 Gbp) in 33,679 contigs
The mean (median) contig size is 10.5 (12.9) kbp
The N50 of the reduced reference is 101,375 bp
The indexed reduced reference genome to use for your mapping is on our server here:
/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa

#### To help make our scripting approach efficient, we’re going to write several short scripts, optimizing each one at a time, then put them together at the end


First, we want to ***specify the population of interest and the paths to the input and output directories***. We can do this by defining variables in bash, like so:

Each student gets assigned a population to work with:

MYPOP="XXXX"

Directory with the cleaned fastq files

INPUT="/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads"

Output dir to store mapping files (bam)

OUT="/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam"

### For mapping, we’ll use the program **bwa**, which is a very efficient and very well **vetted read mapper**. Lots of others exist and can be useful to explore for future datasets. We tried several, and for our exome data, bwa seems to be the best.

### Let’s write a ***bash script called mapping.sh*** that calls the R1 and R2 reads for each individual in your population, and uses the ***bwa-mem2 algorithm to map reads to the reference genome***. We can test this out using one sample (individual) at a time, and then once the syntax is good and the bugs all worked out, we can scale this up to all the inds in our populations.

The basic bwa-mem2 command we’ll use is below. Think about how we should write this into a loop to call all the fastq files for our population of interest…(hint, look back at the fastp.sh script)

/data/popgen/bwa-mem2/bwa-mem2 mem -t 1 ${REF} ${READ1} ${READ2} > ${OUT}${NAME}.sam
where

-t 1 is the number of threads, or computer cpus to use (in this case, just 1)
-${REF} specifies the path and filename for the reference genome
${READ1} specifies the path and filename for the cleaned and trimmed R1 reads 
${READ2} specifies the path and filename for the cleaned and trimmed R2 reads 
>${OUT}/${NAME}.sam  specifies the path and filenam for the .sam file to be saved into a new directory
Other bwa options detailed here: bwa manual page
Because you’re each mapping sequences from multiple samples (N=8/pop), it’s going to take a little while.

### Whenever you have a job that will take a long time, you’re going to want to start a “screen session” using the tmux command. ***tmux initiates a new shell window that won’t interrupt or stop your work if you close your computer, log off the server, or leave the UVM network***. Anytime you’re running long jobs, you definitely want to use tmux.

Using it is easy. Here’s what you need to do:

Type tmux and hit <return>

You are now in a “screen” — meaning you can start a job and once you exit the screen it will keep running.

Start your program or run your bash script like so: bash mapping.sh

### ***You’ll see the program start running. - Assuming everything is working and you’re not seeing errors, you want to now detach from the screen. To do so, hold the <control> key down while you hit the b key. Then release the <control> key and hit just the d key.***

##### If you do it right, then you should exit your screen session and go back to a “regular” terminal. If you don’t do this, you’ll lose your work!

#### You can recover your screen by typing ***tmux attach***. That’ll re-attach you back to your program!

When your script is ready, do:

tmux

bash mapping.sh
Don’t forget to detach from your screen!

While that’s running, let’s take a look at a *** Sequence AlignMent (SAM) file*** already available in /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/
First, try looking at a SAM file using head and tail.
tail -n 100 FILENAME.sam


### A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, there’s a line in the SAM file that includes

- the read, aka. query, name,

- a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),

- the reference sequence name to which the read mapped

- the leftmost position in the reference where the read mapped

- the mapping quality (Phred-scaled)

- a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))

- an ‘=’, mate position, inferred insert size (columns 7,8,9),

- the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),

- then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), - the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).

- The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the ‘@’ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.

Find the official Sequence AlignMent file documentation can be found here or more officially.

Some useful FLAGs to know - for example what do the numbers in the second column of data mean?

Here’s a SAM FLAG decoder by the Broad Institute.

### How can we get a summary of how well our reads mapped to the reference?

#### We can use the program ***sambamba for manipulating sam/bam files***. sambamba. Sambamba is closely related to its progenitor program samtools which is written by the same scientist who develop bwa, Heng Li. sambamba has been re-coded to increase efficiency (speed).

First we have to convert the sam file to bam format:

sambamba view -S -f bam IN.sam -o OUT.bam

Then we can use the command flagstats gets us some basic info on how well the mapping worked:

sambamba flagstat FILENAME.bam 

------   

<div id="id-section3"></div>

## Entry 3: 2023-09-18.

Learning Objectives for 09/18/23
Introduce lab notebooks
Visualize sequence alignment files (*.sam)
Process the mapping file *sam to binary (*.bam), sort, and remove duplicate reads
Calculate mapping statistics to assess quality of the result
1. Lab Notebooks: Take notes on your workflow today so you can remember what you did for the future!
An important step is taking good notes on your workflow so you can remember what you did down the road (your “future self”) and share your process with others (reproducible science!). It’s also really important once you start making detailed decisions that will affect the analysis outcome of your data, so you can recreate the results and explore the effect of different assumptions/decisions.

We want you to keep such a notebook for each module in the course (kind of like you would for each experiment, or each thesis chapter you will work on). We’ve provided you with a notebook template based on the markdown (md) language.

Notebook template in markdown (md) format

You should save a copy of this template to your github repo and rename it PopulationGenomics_Notebook.md in your main repo folder. You can then open it up and edit this file directly within RStudio, taking notes using either the Source interface if you know the markdown language (or want to learn) or you can use RStudio’s built-in markdown GUI editor under the Visual tab. This works very similar to Word.

Here’s a cheatsheet for markdown language if you want to write using Source code:

Markdown cheatsheet

When you’re done your entries, save and quit RStudio, then commit your changes and push to Github. You can check out your notebook on your repo’s site, and Github will automatically render the markdown code into a nice format!

2. Visualize the mapping: By now, you should all have Sequence AlignMent (SAM) files for the inds in your populations!
Let’s take a peek at one of the non-binary (sam) alignment files
/netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/

First, try looking at a SAM file using head and tail. Pick one of your files (just one) to play with below:
tail -n 2 YOURFILENAME.sam
A SAM file is a tab delimited text file that stores information about the alignment of reads in a FASTQ file to a reference genome or transcriptome. For each read in a FASTQ file, there’s a line in the SAM file that includes

the read, aka. query, name,
a FLAG (number with information about mapping success and orientation and whether the read is the left or right read),
the reference sequence name to which the read mapped
the leftmost position in the reference where the read mapped
the mapping quality (Phred-scaled)
a CIGAR string that gives alignment information (how many bases Match (M), where there’s an Insertion (I) or Deletion (D))
an ‘=’, mate position, inferred insert size (columns 7,8,9),
the query sequence and Phred-scaled quality from the FASTQ file (columns 10 and 11),
then Lots of good information in TAGS at the end, if the read mapped, including whether it is a unique read (XT:A:U), the number of best hits (X0:i:1), the number of suboptimal hits (X1:i:0).
The left (R1) and right (R2) reads alternate through the file. SAM files usually have a header section with general information where each line starts with the ‘@’ symbol. SAM and BAM files contain the same information; SAM is human readable and BAM is in binary code and therefore has a smaller file size.

Find the official Sequence AlignMent file documentation can be found here or more officially.

Here’s a SAM FLAG decoder by the Broad Institute.

You can look up TAGs specific to bwa mem mapping here

3. Process our mapping files using samtools and sambamba
We can use the program sambamba for manipulating alignment (sam/bam) files. sambamba. Sambamba is closely related to its progenitor program samtools which is written by the same scientist who develop bwa, Heng Li. sambamba has been re-coded to increase efficiency (speed).

There are several steps we need to do:

convert sam alignment file to (binary) bam format
sort the bam file by its read coordinates
mark and remove PCR duplicate reads
index the sorted, duplicate removed alignment for quick lookup
Here’s a script that we can customize for the above jobs: /netfiles/ecogen/PopulationGenomics/scripts/process_bams.sh

Copy that script into your ~/myscripts folder and open it in vim to edit

When you’re ready, enter a screen session using tmux then execute your script bash process_bams.sh. If you get an error that the script doesn’t exist, then either cd into the ~/myscripts directory before running your script, or incorporate the path into the file name when you give your bash command.

Detach from the screen using <CTRL>+b then d. You can always reattach by tmux attach-session

4. Calculate mapping stats: How can we get a summary of how well our reads mapped to the reference?
We can use the program samtools Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.

The samtools command flagstat gets us some basic info on how well the mapping worked

We can also estimate depth of coverage (avg. number of reads/site) using the samtools command depth

We’ll use both of these commands in loops to assess the mapping stats on each sample in our population.

We’ll also use the awk tool to help format the output.

Here’s a script to get us started: /netfiles/ecogen/PopulationGenomics/scripts/bam_stats.sh

If there’s time while that’s running, we can take a look at one of our alignment files (sam or bam) using an integrated viewed in samtools called tview. To use it, simply call the program and command, followed by the sam/bam file you want to view and the path to the reference genome. For example:
samtools tview /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/XXXX_XX_X.sorted.rmdup.bam /netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa



------  

<div id="id-section4"></div>

## Entry 4: 2023-09-20

Learning Objectives for 09/20/23

Finish calculating mapping statistics to assess quality of the result
Introduce use of genotype-likelihoods for analyzing diversity in low coverage sequences
Use the ‘ANGSD’ program to calculate nucleotide diversity (thetas) and neutrality stats
1. Calculate mapping stats: How can we get a summary of how well our reads mapped to the reference?
We can use the program samtools Written by Heng Li, the same person who wrote bwa. It is a powerful tool for manipulating sam/bam files.

The samtools command flagstat gets us some basic info on how well the mapping worked

We can also estimate depth of coverage (avg. number of reads/site) using the samtools command depth

We’ll use both of these commands in loops to assess the mapping stats on each sample in our population.

We’ll also use the awk tool to help format the output. (awk cheatsheet here)

Here’s a script to get us started: /netfiles/ecogen/PopulationGenomics/scripts/bam_stats.sh

2. Inference of population genomics from the aligned sequence data: should we call genotypes?
Many of the papers you’ll read that do popgen on NGS data have a SNP calling step that results in a specific gneotype being called for each SNP site for each individual. For example,

SNP	Ind1	Ind 2
1	CC	CT
2	AG	AA
3	GT	TT
But how do we know that Ind1 is homozygous at SNP-1 (CC) – couldn’t it be CT and we just didn’t have enough coverage to observe the second allele?

The basic problem is that read data are counts that produce a binomial distribution of allele calls at a given site, and if you have few reads, you might by chance not observe the true genotype. So, what’s the right thing to do?

As with almost anything in statistics, the right thing to do is not throw away that uncertainty, but instead incorporate it into your analysis. That’s what we’re going to do…

Genotype-free population genetics using genotype likelihoods
A growing movement in popgen analysis of NGS data is embracing the use of genotype likelihoods to calculate stats based on each individual having a likelihood (probability) of being each genotype.

A genotype likelihood (GL) is essentially the probability of observing the sequencing data (reads containing a particular base), given the genotype of the individual at that site.

These probabilities are modeled explicitly in the calculation of population diversty stats like Theta-pi, Tajima’s D, Fst, PCA, etc…; thus not throwing out any precious data, but also making fewer assumptions about the true (unknown) genotype at each locus

We’re going to use this approach with the program ‘ANGSD’, which stands for ‘Analysis of Next Generation Sequence Data’

This approach was pioneered by Rasmus Nielsen, published originally in Korneliussen et al. 2014.

ANGSD has a user’s manual (it’s a work in progress…)

The basic work flow of ANGSD goes like this:
Create a list of bam files for the samples you want to analyze
Estimate genotype likelihoods (GL’s) and allele frequencies after filtering to minimize noise
Use GL’s to:
estimate the site frequency spectrum (SFS)

estimate nucleotide diversities and neutrality stats (Thetas, Tajima’s D, …)

1. In your ~/myscripts folder, enter vim and create a file called ANGSD.sh
Create the header for your inputs and outputs

mkdir ~/myresults/ANGSD

INPUT=""

OUTPUT=~/myresults/ANGSD

REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

MYPOP=""

ls ${INPUT}/${MYPOP}*sorted.rmdup.bam >${OUTPUT}/${MYPOP}_bam.list
Write (w) and quit(q) your file and try running it at the command line.

Check your output bamlist to see it was made properly!

Where would you look for this file? (Hint, refer back to the ls command that makes it).
How would you verify its contents? (hint: use head or cat or even vim)
2. Open your ANGSD.sh script back up in vim
Estimate your GL’s and allele freqs, optionally filtering for base and mapping quality, sequencing depth, SNP probability, minor allele frequency, etc.

Add the following code chunk at the bottom of your script:

# File suffix to distinguish analysis choices
SUFFIX=""

# Estimating GL's and allele frequencies for all sites with ANGSD

######################

ANGSD -b ${OUTPUT}/${MYPOP}_bam.list \
-ref ${REF} -anc ${REF} \
-out ${OUTPUT}/${MYPOP}_${SUFFIX} \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-GL 1 \
-doSaf 1 \
##### below filters require `do-Counts`
#-doCounts 1 \
#-minInd 4 \
#-setMinDepthInd 1 \
#-setMaxDepthInd 40 \
#-setMinDepth 10 \
#-skipTriallelic 1 \
#-doMajorMinor 1 \
##### below filters require `doMaf`
#-doMaf 1 \
#-SNP_pval 1e-6 \
#-minMaf 0.01
What do all these options mean?

Option	Description
-nThreads 1	how many cpus to use – be conservative
-remove_bads 1	remove reads flagged as ‘bad’ by samtools
-C 50	enforce downgrading of map quality if contains excessive mismatches
-baq 1	estimates base alignment qualities for bases around indels
-minMapQ 20	threshold for minimum read mapping quality (Phred)
-minQ 20	threshold for minimum base quality (Phred)
-GL 1	calculate genotype likelihoods (GL) using the Samtools formula
-doSaf 1	output allele frequency likelihoods for each site
-doCounts 1	output allele counts for each site
-minInd 4	min number of individuals to keep a site (see also ext 2 filters)
-setMinDepthInd 1	min read depth for an individual to count towards a site
-setMaxDepthInd 40	max read depth for an individual to count towards a site
-setMinDepth 10	min read depth across ALL individual to keep a site
-skipTriallelic 1	don’t use sites with >2 alleles
-doMajorMinor 1	fix major and minor alleles the same across all samples
-doMaf 1	calculate minor allele frequencies
-SNP_pval 1e-6	Keep only site highly likely to be polymorphic (SNPs)
-minMaf 0.01	Keep only sites with minor allele freq > some proportion.
NOTES

If you want to restrict the estimation of the genotype likelihoods to a particular set of sites you’re interested in, add the option -sites selected_sites.txt (tab delimited file with the position of the site in column 1 and the chromosome in column 2) or use -rf selected_chromosome.chrs (if listing just the unique “chromosomes” or contigs you want to anlayze)
Some popgen stats you want to estimate only the polymorphic sites; for this you should include the -SNP_pval 1e-6 option to eliminate monomorphic sites when calculating your GL’s
There are good reasons to do it BOTH ways, with and without the -SNP_pval 1e-6 option. Keeping the monomorphic sites is essential for getting proper estimates of nucleotide diversity and Tajima’s D. But other analyses such as PCA or GWAS want only the SNPs.
Write (q) and quit (q) your script. Then run at the command line

3a. In your ~/myscripts folder, create ANGSD_doTheta.sh to estimate the SFS and nucleotide diversity stats for your pop
Based on the saf.idx files from ANGSD GL calls, you can estimate the Site Frequency Spectrum (SFS), which is the precursor to many other analyses such as nucleotide diversities (as well as Fst, demographic history analysis, etc.)

REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

OUT=~/myresults/ANGSD

MYPOP=""

SUFFIX=""

#Estimation of the SFS for all sites using the FOLDED SFS
realSFS ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -maxIter 1000 \
        -tole 1e-6 \
        -P 1 \
        > ${OUT}/${MYPOP}_${SUFFIX}.sfs
3b. After the SFS, estimate the theta diversity stats:
Once you have the SFS, you can estimate the thetas and neutrality stats by adding the following code chunk at the end of your ANGSD_doTheta.sh script:

# Estimate thetas and stats using the SFS

realSFS saf2theta ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -sfs ${OUT}/${MYPOP}_${SUFFIX}.sfs \
        -outname ${OUT}/${MYPOP}_${SUFFIX}

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx
If we wanted to analyze this on sliding windows, we could instead replace the above code chunk with the following:

# For sliding window analysis:

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx \
-win 50000 \
-step 10000 \
-outnames ${OUT}/${MYPOP}_${SUFFIX}.thetasWindow.gz
For either of the results files above, the first column of the results file (*.thetas.idx.pestPG) is formatted a bit funny and we don’t really need it. We can use the cut command to get rid of it:

cut -f2- ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx.pestPG > ${OUT}/${MYPOP}_${SUFFIX}.thetas
This is now ready to bring into R to look at the mean and variability in nucleotide diversity for our pop. How does it compare to others?

We can compare the diversity in our different pops by entering your diversity stats in this google doc:

https://docs.google.com/spreadsheets/d/1y3GMvnGP65fYfBoJsdrixGLkGCe_TcDlo_7GFyPe1QM/edit?usp=sharing


------   

<div id="id-section5"></div>


## Entry 5: 2023-09-25.

Learning Objectives for 09/25/23
Calculate diversity stats for our focal pops (SFS, theta-W, theta-Pi, Tajima’s D)
Summarize the results in R and share to google doc
Introduce Fst in ANGSD using genotype probabilities
1. Calculate SFS and diversity stats
At the end of our last session, we used ANGSD to estimate genotype likelihoods for our red spruce populations. We wrote the script ANGSD.sh to work on these, which should have the following output files in your ~/myresults/ANGSD directory:

-rw-r--r--. 1 kellrlab users 1417463962 Sep 20 11:44 9999_.saf.gz
-rw-r--r--. 1 kellrlab users    1086161 Sep 20 11:44 9999_.saf.idx
-rw-r--r--. 1 kellrlab users   79501025 Sep 20 11:44 9999_.saf.pos.gz
These “saf” files contain “site allele frequency” likelihoods, and are the info needed to estimate stats that depend on population allele frequencies, like nucleotide diversities, neutrality stats like Tajima’s D, and population divergence stats like Fst. Each of these stats depends on the SFS – the Site Frequency Spectrum So, our workflow will be to use the .saf files to estimate the SFS, and then use the SFS to estimate our diversity stats and Fst.

In your ~/myscripts folder, create ANGSD_doTheta.sh to estimate the SFS and nucleotide diversity stats for your pop
Based on the saf.idx files from ANGSD GL calls, first estimate the Site Frequency Spectrum (SFS)

REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

OUT=~/myresults/ANGSD

MYPOP=""

SUFFIX=""

#Estimation of the SFS for all sites using the FOLDED SFS
realSFS ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -maxIter 1000 \
        -tole 1e-6 \
        -P 1 \
        -fold 1 \
        > ${OUT}/${MYPOP}_${SUFFIX}.sfs
After the SFS, estimate the theta diversity stats:
Once you have the SFS, you can estimate the thetas and neutrality stats by adding the following code chunk at the end of your ANGSD_doTheta.sh script:

# Estimate thetas and stats using the SFS

realSFS saf2theta ${OUT}/${MYPOP}_${SUFFIX}.saf.idx \
        -sfs ${OUT}/${MYPOP}_${SUFFIX}.sfs \
        -outname ${OUT}/${MYPOP}_${SUFFIX}

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx
If we wanted to analyze this on sliding windows, we could instead replace the above code chunk with the following:

# For sliding window analysis:

thetaStat do_stat ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx \
-win 50000 \
-step 10000 \
-outnames ${OUT}/${MYPOP}_${SUFFIX}.thetasWindow.gz
An important distinction! The unfolded vs. folded SFS
The big difference here is whether we are confident in the ancestral state of each variable site (SNP) in our dataset

If we know the ancestral state, then the best info is contained in the unfolded SFS, which shows the frequency histogram of how many derived loci are rare vs. common

bins in the unfolded SFS go from 0 to 2N – why?
When you don’t know the ancestral state confidently, you can make the SFS based on the minor allele (the less frequent allele; always < 0.5 in the population).

bins in the folded SFS go from 0 to 1N – why?
Essentially, the folded spectra wraps the SFS around such that high frequency “derived” alleles are put in the small bins (low minor allele freq).



Now we have some diversity results!
For either of the results files above, the first column of the results file (*.thetas.idx.pestPG) is formatted a bit funny and we don’t really need it. There are also a bunch of other neutrality stats that we don’t need right now. We can use the cut command to get rid of these extra columns. The below cut command retains columns 2-5, 9 and 14

cut -f2-5,9,14 ${OUT}/${MYPOP}_${SUFFIX}.thetas.idx.pestPG > ${OUT}/${MYPOP}_${SUFFIX}.thetas
The columns correspond to the following stats:

Col	Statistic	Description
2	Chr	The chromosome or (more appropriately) contig being analyzed
3	WinCenter	The center of the contig, in bp
4	tW	Watterson’s Theta – an estimate of nucleotide diversity based on segregating sites
5	tP	Theta Pi – estimate of nucleotide diversity based on pairwise divergence
9	Tajima	Tajima’s D – a neutrality stat that tests for the difference in tW-tP
14	nSites	The number of base pairs being analyzed along this stretch of cont.
2. Summarize diversity stats in R
We’re now ready to use Filezilla to download these 2 files:

.thetas.idx.pestPG
.sfs
Save these to your results folder in your githuib repo so we can import into R to look at the mean and variability in nucleotide diversity for our pop. Here’s some basic R code to help you along:

setwd("") # set your path to your results folder in your repo where you saved your diversity stats file

list.files() # list out the files in this folder to make sure you're in the right spot.

# First let's read in the diversity stats
theta <- read.table("_.thetas",sep="\t",header=T)

theta$tWsite = theta$tW/theta$nSites #scales the theta-W by the number of sites
theta$tPsite = theta$tP/theta$nSites #scales the theta-Pi by the number of sites

summary(theta)

# You can order the contig list to show you the contigs with the highest values of Tajima's D, or the lowest

head(theta[order(theta$Tajima, decreasing = TRUE),]) # top 10 Tajima's D values

head(theta[order(theta$Tajima, decreasing = FALSE),]) # bottom 10 Tajima's D values

#You can also look for contigs that have combinations of high Tajima's D and low diversity -- these could represent outliers for selection
#theta[which(theta$Tajima>1.5 & theta$tPsite<0.001),]


sfs<-scan('9999_.sfs')
sfs<-sfs[-c(1,which(sfs==0))]
sfs<-sfs/sum(sfs)

# Be sure to replace "9999" with your pop code in the "main" legend below
barplot(sfs,xlab="Chromosomes",
        names=1:length(sfs),
        ylab="Proportions",
        main="Pop 9999 Site Frequency Spectrum",
        col='blue')

# Put the nucleotide diversities, Tajima's D, and SFS into a 4-panel figure
par(mfrow=c(2,2))
hist(theta$tWsite, xlab="theta-W", main="Watterson's theta")
hist(theta$tPsite, xlab="theta-Pi", main="Pairwise Nucleotide Diversity")
hist(theta$Tajima, xlab="D", main="Tajima's D")
barplot(sfs,names=1:length(sfs),main='Site Frequency Spectrum')


# To reset the panel plotting, execute the line below:
dev.off()
We can compare the diversity in our different pops by entering your diversity stats in this google doc:

https://docs.google.com/spreadsheets/d/1y3GMvnGP65fYfBoJsdrixGLkGCe_TcDlo_7GFyPe1QM/edit?usp=sharing

3. Use ANGSD and the SFS for multiple pops to calculate genetic divergence between pops (Fst)
We can calculate Fst between any pair of populations by comparing their SFS to each other. For this, we’ll need to estimate the SFS for pairs of populations; we can each contribute to the overall analysis by looking at how our focal pop is divergent from the others.

For this analysis, let’s calculate Fst between our focal red spruce population (MYPOP) and the black spruce samples…this could tell us which of our pops might be hybridizing. What would we expect for Fst in this case?

Let’s write a bash script called ANGSD_Fst.sh that includes the following code:

# Start with the usual bash script header

# Give yourself some notes

# Path to Black Spruce (BS) input saf.idx data:

BLKSPR="/netfiles/ecogen/PopulationGenomics/fastq/black_spruce/cleanreads/bam/ANGSD"

#Path to save your output:

OUTPUT=

MYPOP=""

SUFFIX=""

# Estimate Fst between my red spruce pop and black spruce:

realSFS ${MYPOP}_.saf.idx ${BLKSPR}/BS_all.saf.idx -P 1 >${MYPOP}_BS.sfs
realSFS fst index ${MYPOP}_.saf.idx ${BLKSPR}/BS_all.saf.idx -sfs ${MYPOP}_BS.sfs -fstout ${MYPOP}_BS -whichFst 1
realSFS fst stats ${MYPOP}_BS.fst.idx 

------
<div id="id-section6"></div>

## Entry 6:2023-09-27

Learning Objectives for 09/27/23
Review the diversity stats for our focal pops on the google doc
Estimate genetic differentiation (Fst) in ANGSD between our focal red spruce pops and black spruce
Visualize population structure using PCA and Admixture
1. Review the diversity stats
Let’s compare the diversity in our different pops on the google doc:

https://docs.google.com/spreadsheets/d/1y3GMvnGP65fYfBoJsdrixGLkGCe_TcDlo_7GFyPe1QM/edit?usp=sharing

I also put map in there for reference so you can see where different pops are located within the range.

What do you notice about the diversities?
Where is Ne the highest/lowest?
What do the average Tajima’s D values suggest about demographic history in these pops?
2. Use ANGSD and the SFS for multiple pops to calculate genetic divergence between pops (Fst)
We can calculate Fst between any pair of populations by comparing their SFS to each other. For this, we’ll need to estimate the SFS for pairs of populations; we can each contribute to the overall analysis by looking at how our focal pop is divergent from the others.

For this analysis, let’s calculate Fst between our focal red spruce population (MYPOP) and the black spruce samples…this could tell us which of our pops might be hybridizing. What would we expect for Fst in this case?

Let’s write a bash script called ANGSD_Fst.sh that includes the following code:

# Start with the usual bash script header

# Give yourself some notes

# Path to Black Spruce (BS) input saf.idx data:

BLKSPR="/netfiles/ecogen/PopulationGenomics/fastq/black_spruce/cleanreads/bam/ANGSD"

OUTPUT=

MYPOP=""

cd ${OUTPUT}

# Estimate Fst between my red spruce pop and black spruce:

realSFS ${MYPOP}_.saf.idx \
        ${BLKSPR}/BS_all.saf.idx \
        -P 1 \
        >${MYPOP}_BS.sfs

realSFS fst index \
        ${MYPOP}_.saf.idx \
        ${BLKSPR}/BS_all.saf.idx \
        -sfs ${MYPOP}_BS.sfs \
        -fstout ${MYPOP}_BS \
        -whichFst 1

realSFS fst stats ${MYPOP}_BS.fst.idx 
Enter the weighted Fst value into the google sheet for your pop. What’s the trend?
3. Visualize popualtion structure across the landscape using PCA and Admixture
We often want to visualize differences in the genetic structure or genetic ancestry in our sample, and lots of papers we’ve read approach this using PCA or Admixture analysis. We can do each of these approaches on genotype likelihoods in ANGSD using a special routine called pcANGSD.

pcANGSD uses a really cool iterative approach where it refines the estimation of allele frequencies for each individual at the same time that it finds the clusters that individual may have ancestry within.



Here are some resources to understand the program options:

The manual page

A nice pcANGSD tutorial that walks through most of the routines

The original paper describing the application to PCA and admixture are here: Meisner & Albrechtsen 2019, Genetics

Since we want to run pcANGSD for the entire set of samples – not just your focal pop – we need the genotype likelihoods from ANGSD for all 95 red spruce samples. That would take a long time to run (about 24 hrs) and would be redundant for each of you to do, so I ran these once for the class.

For your reference (and future work), the code I used to estimate the genotype likelihoods is here: (you don’t have to run this now!)

/netfiles/ecogen/PopulationGenomics/scripts/ANGSD_allRS_poly.sh

and exported the genotype likelihoods in “beagle” format here:

/netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.beagle.gz

We can use the beagle file containing the genotype likelihoods for all 95 red spruce samples as input to pcANGSD. The script is actually not too bad…let’s give it a go:

# Start with the usual bash script header

# Give yourself some notes

# Path to your input data (where the beagle file lives)

INPUT=

# Path to save your output (in your home directory):

OUTPUT=

SUFFIX="allRS_poly"

# Make a copy of the list of bam files for all the red spruce samples and place in your home directory. You'll need this later for making figures.

cp ${INPUT}/allRS_bam.list ${OUTPUT}


# To run pcANGSD, you need to activate a "virtual environment" on the server by including the line below:

source /data/popgen/pcangsd/venv/bin/activate  


# Then, run PCA and admixture scores with pcangsd:

pcangsd -b ${INPUT}/${SUFFIX}.beagle.gz \
        -o ${OUTPUT}/${SUFFIX} \
        -e 1 \
        --admix \
        --admix_alpha 50 \
        --threads 1 
This will run pcANGSD assuming it fits a single “eigenvalue” to split your samples into K=2 clusters. If you want to explore higher levels of clustering in the future, you can include the -e <int> flag, where is a number that is K-1 number of clusters you want to fit.

Once the run is finished, use FileZilla to transfer the following files over to your repo on your laptop:

allRS_bam.list
allRS_poly.cov
allRS_poly.admix.2.Q
When you have these files transferred (don’t forget where you saved them to on your laptop!), open up RStudio and let’s start making some figures!

Just a reminder, the following is R code, not bash. ;)

library(ggplot2) # plotting
library(ggpubr) # plotting

setwd("") # set the path to where you saved the pcANGSD results on your laptop

## First, let's work on the genetic PCA:

COV <- as.matrix(read.table("allRS_poly.cov")) # read in the genetic covariance matrix

PCA <- eigen(COV) # extract the principal components from the COV matrix

## How much variance is explained by the first few PCs?

var <- round(PCA$values/sum(PCA$values),3)

var[1:3]

# A "screeplot" of the eigenvalues of the PCA:

barplot(var, 
        xlab="Eigenvalues of the PCA", 
        ylab="Proportion of variance explained")

## Bring in the bam.list file and extract the sample info:

names <- read.table("allRS_bam.list")
names <- unlist(strsplit(basename(as.character(names[,1])), split = ".sorted.rmdup.bam"))
split = strsplit(names, "_")
pops <- data.frame(names[1:95], do.call(rbind, split[1:95]))
names(pops) = c("Ind", "Pop", "Row", "Col")

## A quick and humble PCA plot:

plot(PCA$vectors[,1:2],
     col=as.factor(pops[,2]),
     xlab="PC1",ylab="PC2", 
     main="Genetic PCA")
     
## A more beautiful PCA plot using ggplot :)

data=as.data.frame(PCA$vectors)
data=data[,c(1:3)]
data= cbind(data, pops)

cols=c("#377eB8","#EE9B00","#0A9396","#94D2BD","#FFCB69","#005f73","#E26D5C","#AE2012", "#6d597a", "#7EA16B","#d4e09b", "gray70")

ggscatter(data, x = "V1", y = "V2",
          color = "Pop",
          mean.point = TRUE,
          star.plot = TRUE) +
  theme_bw(base_size = 13, base_family = "Times") +
  theme(panel.background = element_blank(), 
        legend.background = element_blank(), 
        panel.grid = element_blank(), 
        plot.background = element_blank(), 
        legend.text=element_text(size=rel(.7)), 
        axis.text = element_text(size=13), 
        legend.position = "bottom") +
  labs(x = paste0("PC1: (",var[1]*100,"%)"), y = paste0("PC2: (",var[2]*100,"%)")) +
  scale_color_manual(values=c(cols), name="Source population") +
  guides(colour = guide_legend(nrow = 2))




## Next, we can look at the admixture clustering:

# import the ancestry scores (these are the .Q files)

q <- read.table("allRS_poly.admix.2.Q", sep=" ", header=F)

K=dim(q)[2] #Find the level of K modeled

## order according to population code
ord<-order(pops[,2])

# make the plot:
barplot(t(q)[,ord],
        col=cols[1:K],
        space=0,border=NA,
        xlab="Populations",ylab="Admixture proportions",
        main=paste0("Red spruce K=",K))
text(tapply(1:nrow(pops),pops[ord,2],mean),-0.05,unique(pops[ord,2]),xpd=T)
abline(v=cumsum(sapply(unique(pops[ord,2]),function(x){sum(pops[ord,2]==x)})),col=1,lwd=1.2)
What have we learned about the genetic structure from the PCA and admixture plots?
What does the PCA seem to be telling us?

What different picture does the admixture plot reveal? How does it relate to the PCA?

Would we want to look for higher levels of K in the admixture analysis? How do we do that??

For reference, here’s a map of the sample sites within the red spruce range:



------   
<div id="id-section7"></div>

## Entry 7: 2020-10-02.

Learning Objectives for 10/02/23 Part 1
- Review the population structure results
- Perform a scan for selection and identify contigs with outlier loci
- Identify and visualize outlier loci

1. What have we learned about the genetic structure from the PCA and admixture plots?
Let’s take a look at the PCA and admixture figures posted to the Slack #coding channel.

What does the PCA seem to be telling us?

What different picture does the admixture plot reveal? How does it relate to the PCA?

Would we want to look for higher levels of K in the admixture analysis? How do we do that??

2. How has selection acted to drive divergence of individual loci in excess of the population structure we’ve observed?
When selection acts in response to local environmental conditions, we observe an excess of population structure at certain loci. This can be thought of as Fst at a single locus exceeding some background level of divergence that exists across the genome as a whole. We call these “Fst outliers” and identifying these outliers is a major goal in ecological genomic studies. But, a major challenge is how best to control for background population structure – in other words, how can we sift out the “outliers” from the rest of the population structure in the genome?

One approach to identifying Fst outliers is using genetic PCA to (a) identify the major axes of population structure, and then (b) find loci that “load” very strongly on these axes, indicating their exceptional divergence is probably driven by the action of selection in excess of genetic drift.

We can run a scan for Fst outliers using pcANGSD, just like we did for the genetic PCA, but here we initiate a selection scan for loci that are exceptionally divergent along one or more of the inferred genetic PC axes. This method is especially helpful when it’s hard to define what are genetic “populations”, since it uses the genetic PCA to infer the genetic structure directly. The method employed in pcANGSD follows the “FastPCA” selection scan method described by Galinsky et al. 2015

The approach essentially uses the SNP weights or “loadings” on a given genetic PC axis to determine the strength of association, and if it exceeds the expectation due to neutral drift.

Let’s write a bash script called pcANGSD_selection.sh that includes the following code. Recall that pcANGSD uses genotype likelihoods in “beagle” format, which have been calculated for you (see last tutorial) and can be found here: /netfiles/ecogen/PopulationGenomics/ANGSD/

Here’s a start to our script:

INPUT=""

OUTPUT=

SUFFIX="allRS_poly"

cp ${INPUT}/allRS_bam.list ${OUTPUT}

# Activates the pcANGSD environment and run the selection scan:

source /data/popgen/pcangsd/venv/bin/activate
        
pcangsd -b ${INPUT}/${SUFFIX}.beagle.gz \
    -o ${OUTPUT}/${SUFFIX} \
    -e <what number should we use here?>
    --selection \
    --minMaf 0.05 \
    --sites_save \
    --snp_weights \
    --threads 1
    
Note the minMaf 0.05 option tests only those loci that have a minor allele frequency of 0.05 of greater. That’s because there’s little statistical power for testing association of alleles that are very rare (<5% in the sample).

It should run quickly! Once finished, you’ll the see a new set of files, including:

File name	contents
allRS_poly.selection.npy	selection scores for each locus on each tested PC axis
allRS_poly.weights.npy	weights that show how strongly each locus “loads” on each PC axis
allRS_poly.sites	for each locus shows whether it got tested (1) or not (0)
You’ll also want to get a file with minor allele frequencies (“maf”) that ANGSD makes when first estimating genotype likelihoods. You have one of these files from when you worked on your focal pops, but we need one for the “allRS_poly” run of all red spruce individuals for just polymorphic sites. I’ve provided this here:

/netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.mafs.gz

We need to combine the “mafs” file and the “sites” file to create a file that has the contig and position info for each locus that got tested for selection. Here’s a few lines of bash code to create this file:

First, we need to add a header to the “sites” file:
sed -i '1i kept_sites' ~/myresults/ANGSD/allRS_poly.sites

Next, we need to get the contig and position info out of the “mafs” file and combine it with the “sites” file:
zcat /netfiles/ecogen/PopulationGenomics/ANGSD/allRS_poly.mafs.gz | cut -f1-7 | paste - ~/myresults/ANGSD/allRS_poly.sites >allRS_poly_mafs.sites

Now we have all the files we need! Be sure to transfer the 3 listed above, plus the new allRS_poly_mafs.sites you just made, using FileZilla and save into your repo on your laptop. When you have these files transferred (don’t forget where you saved them to on your laptop!), open up RStudio and let’s start making some figures!

3. Idenitfy and visualize outlier loci
We’ll use R to import the selection scan results and associated meta-data, assign p-values for each tested locus, and visualize the results!

Just a reminder, the following is R code, not bash. ;)


###################################
#  Selection scans for red spruce #
###################################

library(RcppCNPy) # for reading python numpy (.npy) files

setwd("~/Documents/Github/Ecological_Genomics/Fall_2023/pcangsd/")

list.files()

### read in selection statistics (these are chi^2 distributed)

s<-npyLoad("allRS_poly.selection.npy")

# convert test statistic to p-value
pval <- as.data.frame(1-pchisq(s,1))
names(pval) = "p_PC1"

## read positions
p <- read.table("allRS_poly_mafs.sites",sep="\t",header=T, stringsAsFactors=T)
dim(p)

p_filtered = p[which(p$kept_sites==1),]
dim(p_filtered)

# How many sites got filtered out when testing for selection? Why?

## make manhattan plot
plot(-log10(pval$p_PC1),
  col=p_filtered$chromo,
  xlab="Position",
  ylab="-log10(p-value)",
  main="Selection outliers: pcANGSD e=1 (K2)")

# We can zoom in if there's something interesting near a position...

plot(-log10(pval$p_PC1[2e05:2.01e05]),
  col=p_filtered$chromo, 
  xlab="Position", 
  ylab="-log10(p-value)", 
  main="Selection outliers: pcANGSD e=1 (K2)")

# get the contig with the lowest p-value for selection
sel_contig <- p_filtered[which(pval==min(pval$p_PC1)),c("chromo","position")]
sel_contig

# get all the outliers with p-values below some cutoff
cutoff=1e-3   # equals a 1 in 5,000 probability
outlier_contigs <- p_filtered[which(pval<cutoff),c("chromo","position")]
outlier_contigs

# how many outlier loci < the cutoff?
dim(outlier_contigs)[1]

# how many unique contigs harbor outlier loci?
length(unique(outlier_contigs$chromo))
How do we find out what functional genes are contained on the unique contigs harboring outliers?

We can use the Picea abies reference genome annotation to get the genes based on the outlier contigs, then test for enrichment of gene function using available public databases.

First, export your unique outlier contigs in R:

write.table(unique(outlier_contigs$chromo),
  "allRS_poly_PC1_outlier_contigs.txt", 
  sep="\t",
  quote=F,
  row.names=F,
  col.names=F)
Transfer back to the server and then grep out the gene IDs. We can do this with a series of piped commands like so:

Define your paths to the ref genome annotation on the server and your newly made outlier loci file

Use zcat to open the annotation without unzipping it, then pipe to grep and use the -f flag to take the contig names from the outliers file – this is a handy trick, and saves us a lot of time from having to manually input each contig one at a time and search!

Pipe to a new grep to get just the contigs containing genes

Pipe the list of gene IDs to take just the unique ones, cut out the 9th column (containing the gene IDs), and get rid of the annoying “ID=” portion of each gene ID

Just a reminder, the following is bach code, not R ;)

ANNOT="/netfiles/ecogen/PopulationGenomics/ref_genome/annotation/Pabies1.0-all-cds.gff3.gz"

OUTLIERS=~/myresults/ANGSD/allRS_poly_PC1_outlier_contigs.txt

zcat ${ANNOT} | grep -f ${OUTLIERS} | grep "gene" | uniq | cut -f9 | sed "s/ID=//g"
Voila! You should now have a list of gene IDs printed to your screen. You can either copy these by highlighting with your mouse and clicking or you can save to an external file for later (using the >outfile.txt command)

Take your gene IDs and go to the plantgenie website. Here, you can create a gene list using your copied IDs, determine which genes they correspond to (not all wil be annotated!), and test for functional enrichment:

In the upper left corner, see a circle with 3 red bars –> click it, and create a new gene list

Paste your gene IDs into the list, and plantgenie will search the P. abies annotation and return all the known genes

Click the “+” button to save these to the current gene list, then close out

Go to the “Analysis Tools” tab on the top bar of the page, and choose “Enrichment”

Plantgenie will now test for whether the annotation Gene Ontology (GO) and Protein Family (Pfam) functional descriptions assigned to each gene ID are over-represented compared to all other genes containing those GO and Pfam categories in the P. abies genome.

Relate back to the biology!

What sort of functional genes did you find are represented in your outliers?
Are any enriched for certain GO or Pfam functions? (focus on p-values << 0.01, or q-values < 0.05)
Remember the population structure axis (genetic PC) these loci are outliers along…are any of the enriched functions interesting in light of this structure? Any surprising?


------   
<div id="id-section7"></div>

## Entry 8: 2020-10-04.

Learning Objectives for 10/02/23 Part 2
Extract climate data for red spruce localities and summarize with PCA
Start genotype-environment association (GEA) analysis running
We’ve now learned about outlier loci that show extreme population structure along genetic PC axes. These are loci that tend to have high Fst (population structure) suggestive of selection and local adaptation. But how does this relate to different aspects of the environment? How do we know what environmental gradients might be responsible for driving this population differentiation? Where’s the “landscape” in all this genomics stuff?

That’s where GEA comes in – short for “Genotype-Environment Association”. The whole idea behind GEA is to look for loci that show strong associations between allele frequencies and environmental gradients, like climate. There’s a substantial literature on GEA methods in landscape genomics, but a good background paper is Rellstab et al. (2015).

For any GEA analysis, you essentially need two things:

Genotype data from individuals sampled across one or more environmental gradients. Check!
Environmental data (like cimate) you want to test as drivers of the selection on allele frequencies. Let’s go get some!
1. Getting Bioclim climate data for our red spruce samples
On our laptops, we’ll use R to query the Worldclim climate database and pull out the bioclim variables for each of our samples based on their lat/longs.

Remember, the following code is in R, not bash ;)

# You made need to use "install.packages" if you don't have some of the below libraries already

library(raster)
library(FactoMineR)
library(factoextra)
library(corrplot)

setwd("")

bio <- getData("worldclim",var="bio",res=10)

coords <- read.csv("https://www.uvm.edu/~kellrlab/forClass/colebrookSampleMetaData.csv", header=T)

The chunk below refers to your bamlist file that you transferred during last week's PCA/admixture analysis.  It should be the same one you want to use here -- if your sample list for analysis changes in the future, you'll need a different bamlist!

names <- read.table("pcangsd/allRS_bam.list")
names <- unlist(strsplit(basename(as.character(names[,1])), split = ".sorted.rmdup.bam"))
split = strsplit(names, "_")
pops <- data.frame(names[1:95], do.call(rbind, split[1:95]))
names(pops) = c("Ind", "Pop", "Row", "Col")

angsd_coords <- merge(pops, coords, by.x="Ind", by.y="Tree")

points <- SpatialPoints(angsd_coords[c("Longitude","Latitude")])

clim <- extract(bio,points)

angsd_coords_clim <- cbind.data.frame(angsd_coords,clim)
str(angsd_coords_clim)
And just like that, we’ve got Bioclim data for our samples!
Now, we don’t want to test all 19 variables in our GEA, but rather just one or a few that seem to capture the climate gradients in our samples. For this, we can use PCA on the climate data (so, an environmental, not a genetic PCA in this case). Using the cliamte PCA can then let us see which variables most strongly define the climate space of red spruce, and we can then use these for our GEA test.

In R:
clim_PCA = PCA(angsd_coords_clim[,15:33], graph=F)

# screeplot of PCA eigenvalues
fviz_eig(clim_PCA)

# plot the variable loadings on the 1st 2 PCs
fviz_pca_var(clim_PCA)

# Which variables show the strongest loading on each axis?
var <- get_pca_var(clim_PCA)
corrplot(var$cos2, is.corr=FALSE)
  
# We can ask the same question using correlation tests of each variable with each PC axis:
dimdesc(clim_PCA)

# Lastly, how do our samples fall out in climate PCA space?
fviz_pca_ind(clim_PCA, 
             geom.ind="point",
             col.ind = angsd_coords_clim$Latitude, 
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             legend.title="Latitude")
What do you think? Let’s pick 1 Bioclim variabe for each PC1 and 2 axis and export the values:

write.table(scale(angsd_coords_clim[,c("bioXX","bioYY")]),
            "allRS_bioXX_YY.txt",
            sep="\t",
            quote=F,
            row.names = F,
            col.names=F)
After you’ve written this file out, use FileZilla to transfer it to the server and into your ~/myresults/ANGSD folder.

You’ll then need to use the cut command to write out separate environmental files for each variable (ANGSD only tests one variable at a time).

Can you think of a cut statement to take your allRS_bioXX_YY.txt file as input and write out a new output called allRS_bioXX.txt that has just the first variable in it? Then try it again for the 2nd variable.

cut options someinfile >someoutfile

2. Getting our GEA run on the server started!
The GEA analysis will likely take some time – maybe over a day to run. So let’s get it started now so we can have the results to look at on Wednesday.

We’ll use ANGSD again to test for associations between allele frequencies and the bioclim variables while using genotype likelihoods to account for uncertainty. The ANGSD routine we’ll use is called doAsso which stands for “do Association”. It has it’s own manual page on ANGSD’s website

AGNSD doAsso requires genotype probabilities (yes, these are slightly different mathematical than genotype likelihoods), which means we’ll need to call the initial set of ANGSD commands to import the bam’s and filter them before sending the probabilities to the doAsso function.

bash header

notes


REF="/netfiles/ecogen/PopulationGenomics/ref_genome/Pabies1.0-genome_reduced.fa"

SUFFIX=""

INPUT=""

OUTPUT=

# Make a list of all the sorted and PCR dup removed bam files:

ls /netfiles/ecogen/PopulationGenomics/fastq/red_spruce/cleanreads/bam/2*sorted.rmdup.bam > ${OUTPUT}/allRS_bam.list

# Run ANGSD to estimate the Geno Probs and then perform the GEA using doAsso:

ANGSD -b ${OUTPUT}/allRS_bam.list \
-ref ${REF} -anc ${REF} \
-out ${OUTPUT}/${SUFFIX} \
-nThreads 1 \
-remove_bads 1 \
-C 50 \
-baq 1 \
-minMapQ 20 \
-minQ 20 \
-GL 1 \
-doSaf 1 \
-doCounts 1 \
-minInd 47 \
-setMinDepthInd 1 \
-setMaxDepthInd 40 \
-skipTriallelic 1 \
-doMajorMinor 1 \
-doMaf 1 \
-SNP_pval 1e-6 \
-minMaf 0.05 \
-doPost 1 \
-doAsso 5 \
-yQuant ${OUTPUT}/allRS_bioXX.txt
That’s a lot of options for ANGSD! Here’s the breakdown (after the obvious in/out and reference genome at the beginning):

Code option	Meaning
-nthreads	how many CPU’s you want to use
-remove_bads	discards reads that don’t map as a pair to a unique location in the ref genome
-C 50	downgrades the quality of reads with excessive mismatches to the ref genome
-baq 1	performs re-calibration of base Q-scores around regions of structural variation
-minMapQ 20	discards reads with mapping Q-score <20
-minQ 20	sets bases with Q-scores < 20 to missing
-GL 1	estimate genotype likelihoods using the Samtools algorithm
-doSAF 1	estimate site-allele frequencies
-doCounts 1	count mapped bases at each site – needed for filters below
-minInd 47	skip loci that have < minInd number of individuals with data; set to ~50% of your full sample size
-setMinDepthInd 1	set individuals with <1 read per site to missing for that site
-setMaxDepthInd 40	set individuals with > 40 reads per site to missing for that site
-skipTriallelic 1	if there are >2 alleles present at a site – discard!
-doMajorMinor 1	calculate which allele is the most common (major) and which is rare (minor)
-doMaf 1	compute the minor allele frequencies
-SNP_pval 1e-6	discard sites unlikely to be polymorphic, based on a p-value of <1e-6
-minMaf 0.05	discard sites with minor allele frequencies < 5%
Most of these are identical to what I used for calling the genotype likelihoods (beagle files) that we used as input into pcANGSD. But I wanted to give you all the options and their meaning (see also the ANGSD manual online so you could have them in case you need to change anything.

Those last 3 lines are the important part for the GEA:

Code option	Meaning
doPost 1	compute genotype probabilities
doAsso 5	perform the GEA association test
-yQuant	path and filename containing the input environmental variable to test
You’ll also notice that we do this just on loci that are present in at least 5% frequeny across the entire set of samples (-minMaf 0.05)

Save your script as ANGSD_GEA.sh

Then you know what to do….tmux, bash, dettach from screen, top to see if you’re running!

